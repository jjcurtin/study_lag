{
  "hash": "a6e44577f26e07fa1ef8eb7eabbd9e48",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Forecasting Risk of Alcohol Lapse up to Two Weeks in Advance using Time-lagged Machine Learning Models\nauthor:\n  - name: Kendra Wyant \n    corresponding: false\n    affiliations:\n      - Department of Psychology, University of Wisconsin-Madison\n  - name: Gaylen E. Fronk \n    corresponding: false\n    affiliations:\n      - Department of Psychology, University of Wisconsin-Madison\n      - Department of Psychiatry and Behavioral Sciences, Medical University of South Carolina\n  - name: Jiachen Yu\n    corresponding: false\n    affiliations:\n      - Department of Psychology, University of Wisconsin-Madison\n  - name: Claire E. Punturieri\n    corresponding: false\n    affiliations:\n      - Department of Psychology, University of Wisconsin-Madison\n  - name: John J. Curtin \n    corresponding: true\n    email: jjcurtin@wisc.edu\n    affiliations:\n      - Department of Psychology, University of Wisconsin-Madison \nfinancial-support: This research was supported by grants from the National Institute on Alcohol Abuse and Alcoholism (NIAAA; R01 AA024391 to John J. Curtin) and the National Institute on Drug Abuse (NIDA; R01 DA047315 to John J. Curtin).\nkeywords:\n  - Substance use disorders\n  - Precision mental health \nabstract: |\n  We developed machine learning models to predict future alcohol lapses within 24-hour windows lagged 1 day, 3 days, 1 week, and 2 weeks. We engineered features from 4x daily ecological momentary assessment from individuals (N=151; 51% male; mean age=41; 87% non-Hispanic White) in early recovery over three months. We trained and evaluated models using nested cross-validation. Median posterior auROC was high (0.85–0.91) for all models but decreased modestly with increasing lag. Models performed worse for non-advantaged groups (non-White and/or Hispanic, below poverty, female). Past alcohol use, abstinence self-efficacy, and craving were the most important features, with the magnitude of importance varying meaningfully by lag. These findings demonstrate feasibility of predicting next-day lapses up to two weeks in advance. Embedding these models in a recovery monitoring support system could enable adaptive, personalized care. Improving model fairness and optimizing the delivery of model feedback to sustain engagement remain critical next steps. \ndate: last-modified\nnumber-sections: false \neditor_options: \n  chunk_output_type: console\nformat:\n  html: default\n  docx: default\n  apaish-typst: \n    documentmode: man\nciteproc: true\nbibliography: references.bib\ncsl: plos-one.csl\n---\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(source(\"https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true\"))\n\npath_models_lag <- format_path(\"risk/models/lag\")\npath_processed <- format_path(\"risk/data_processed/lag\")\n\noptions(knitr.kable.NA = '')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# read in tibbles for in-line code\n\ntest_metrics_all_pp_perf <- read_csv(here::here(path_models_lag, \n                                                \"pp_perf_tibble.csv\"), \n                                     col_types = cols())\n\nci_baseline <- read_csv(here::here(path_models_lag, \"contrast_baseline.csv\"), \n                        col_types = cols())\n\nci_lag <- read_csv(here::here(path_models_lag, \"contrast_adjacent.csv\"), \n                   col_types = cols())\n\npp_dem <- read_csv(here::here(path_models_lag, \"pp_dem_all.csv\"), col_types = cols())\n\npp_dem_contrast <- read_csv(here::here(path_models_lag, \"pp_dem_contrast_all.csv\"), col_types = cols())\n\nscreen <- read_csv(here::here(path_processed, \"dem_tibble.csv\"),\n                   col_types = cols())\n\nlapses_per_subid <- read_csv(here::here(path_processed, \"lapse_tibble.csv\"),\n                             col_types = cols())\n```\n:::\n\n\n\n\n\n# Introduction\n\nAlcohol and other substance use disorders (SUDs) are serious chronic conditions, characterized by high relapse rates [@mclellanDrugDependenceChronic2000; @dennisManagingAddictionChronic2007], substantial co-morbidity with other physical and mental health problems [@substanceabuseandmentalhealthservicesadministration2023NSDUHDetailed; @dennisManagingAddictionChronic2007], and an increased risk of mortality [@hedegaardDrugOverdoseDeaths2021; @centersfordiseasecontrolandpreventioncdcAnnualAverageUnited]. Too few individuals receive medications or clinician-delivered interventions to help them initially achieve abstinence and/or reduce harms associated with their use [@substanceabuseandmentalhealthservicesadministration2023NSDUHDetailed].  Moreover, this problem is even worse for subsequent continuing care during SUD recovery. Continuing care, including both risk monitoring and ongoing support, is the gold standard for managing chronic health conditions such as diabetes, asthma, and HIV [@wagnerImprovingChronicIllness2001]. Yet, continuing care for SUDs is largely lacking despite ample evidence that SUDs are chronic, relapsing conditions [@substanceabuseandmentalhealthservicesadministration2023NSDUHDetailed; @stanojlovicTargetingBarriersSubstance2021; @sociasAdoptingCascadeCare2016].\n\nAn important focus of continuing care during SUD recovery is to prevent lapses (i.e., single instances of goal-inconsistent substance use) and full relapse back to harmful use [@marlattRelapsePreventionMaintenance1985; @witkiewitzRelapsePreventionAlcohol2004].  Critically, the risk factors that instigate lapses during recovery are individualized, numerous, dynamic, interactive, and non-linear [@witkiewitzModelingComplexityPosttreatment2007; @brandonRelapseRelapsePrevention2007]. The optimal supports to address these risk factors and encourage continued, successful recovery vary both across individuals and within an individual over time. Given this, continuing care could benefit greatly from a precision mental health approach that seeks to provide the right support to the right individual at the right time, every time [@bickmanAchievingPrecisionMental2016; @derubeisHistoryCurrentStatus2019; @kranzlerPersonalizedTreatmentAlcohol2012]. However, such monitoring and personalized support must also be highly scalable to address the substantial unmet need for SUD continuing care.\n\nRecent advances in both smartphone sensing [@mohrPersonalSensingUnderstanding2017] and machine learning [@hastieElementsStatisticalLearning2009] hold promise as a scalable foundation for monitoring and personalized support during SUD recovery. Smartphone sensing approaches (e.g., ecological momentary assessment [EMA], geolocation sensing) can provide the frequent, longitudinal measurement of proximal risk factors that is necessary for prediction of future lapses with high temporal precision. EMA may be particularly well-suited for lapse prediction because it can provide privileged access to subjective experiences (e.g., craving, affect, stress, motivation, self-efficacy) that are targets for change in evidence-based approaches for relapse prevention [@marlattRelapsePreventionMaintenance1985; @witkiewitzRelapsePreventionAlcohol2004; @bowenMindfulnessBasedRelapsePrevention2021]. Furthermore, individuals with SUDs have found EMA to be acceptable for sustained measurement for up to a year with relatively high compliance [@wyantAcceptabilityPersonalSensing2023; @moshontzProspectivePredictionLapses2021], suggesting that this method is feasible for long-term monitoring throughout SUD recovery.\n\nMachine learning models are well-positioned to use EMAs as inputs to provide temporally precise prediction of the probability of future lapses with sufficiently high performance to support decisions about interventions and other supports for specific individuals.  These models can handle the high dimensional feature sets that may result from feature engineering densely sampled raw EMA over time [@wyantMachineLearningModels2024]. They can also accommodate non-linear and interactive relationships between features and lapse probability that are likely necessary for accurate prediction of lapse probability.  Moreover, rapid advances in the tools for interpretable machine learning (e.g., Shapley values [@lundbergUnifiedApproachInterpreting2017]) now allow us to probe these models to understand which risk features contribute most strongly to a lapse prediction for a specific individual at a specific moment in time. Interventions, supports, and/or suggested lifestyle adjustments can then be personalized to address these risks following from our understanding about relapse prevention. \n\nPreliminary research is now emerging that uses features derived from EMAs in machine learning models to predict the probability of future alcohol use [@soysterPooledPersonspecificMachine2022; @waltersUsingMachineLearning2021; @wyantMachineLearningModels2024].  This research is important because it rigorously required strict temporal ordering necessary for true prediction, with features measured before alcohol use outcomes. These studies also used resampling methods (e.g., cross-validation) that prioritize model generalizability to increase the likelihood these models will perform well with new people. Perhaps most importantly, @wyantMachineLearningModels2024 demonstrated that machine learning models using EMA can provide predictions with very high temporal precision at clinically implementable levels of performance.  Specifically, we developed models that predict lapses in the immediate future (i.e., the next day and even the next hour) with areas under the receiver operating characteristic curve (auROCs) of 0.91 and 0.93, respectively. \n\n@wyantMachineLearningModels2024's next day lapse prediction model can provide personalized support recommendations to address immediate risks for possible lapses.  Features derived from past EMAs can be updated in the early morning to yield the predicted lapse probability for an individual that day.  Personalized supports that target the top features contributing to that prediction can then be provided.  For example, if predicted lapse probability is high due to frequent craving, the individual could be reminded about the benefits of urge surfing or distracting activities during brief periods when cravings arise.  Conversely, guided relaxation techniques could be recommended if lapse probability is high due to recent past and anticipated stressors that day. Patients could also be assisted to implement any of these recommendations using videos or other tools within a digital therapeutic.  Curtin and colleagues are currently evaluating outcomes associated with the use of this \"smart\" (machine learning guided) recovery monitoring and support system (RMSS) for patients with an alcohol use disorder (AUD) [@wyantMaximizingEngagementTrustunderreview].  \n\nDespite the promise offered by a smart RMSS based on immediate future risks (e.g., the next day), such a system has limitations.  Most importantly, recommendations must be limited to previously learned skills and/or supports that are available to implement that day.  However, many risks may require supports that are not available in the moment.  For example, to address lifestyle imbalances, several future positive activities may need to be planned. Time with supportive friends or an AA sponsor may require time to schedule.  Similarly, work or family schedules may need to be adjusted to return to attending self-help meetings.   If new recovery skills or therapeutic activities are needed to address emerging risks, patients may need to book sessions with a therapist.  In all these instances, patients would benefit from advanced warning about changes in their lapse probability and the associated risks that contribute to these changes.  A smart RMSS could provide this advanced warning by lagging lapse probability predictions further into the future (e.g., predicting lapse probability in a 24-hour window that begins two weeks in the future).  However, we do not know if such lagged models could maintain adequate performance for clinical implementation. \n\nIn this study, we evaluated the performance of machine learning models that predict the probability of future lapses within 24-hour prediction windows that were systematically lagged further into the future.  We considered several meaningful lags for these prediction windows: 1 day, 3 days, 1 week, and 2 weeks.  We conducted pre-registered analyses of both the absolute performance of these lagged models and their relative performance compared to a baseline model that predicted lapse probability in the immediate next day (i.e., no lag).  In addition to the aggregate performance of these models, we also evaluated algorithmic fairness by comparing model performance across important subgroups that have documented disparities in treatment access and/or outcomes.  These include comparisons by race/ethnicity [@pinedoCurrentReexaminationRacial2019; @kilaruIncidenceTreatmentOpioid2020], income [@olfsonHealthcareCoverageService2022] and sex at birth [@greenfieldSubstanceAbuseTreatment2007; @kilaruIncidenceTreatmentOpioid2020]. Finally, we calculated Shapley values for feature categories defined by EMA items to better understand how these models generate predictions and how these features can be used to tailor personalized supports. \n\n\n# Methods\n\n## Transparency\n\nWe adhere to research transparency principles that are crucial for robust and replicable science. We preregistered our data analytic strategy. We reported all data exclusions. Our data, questionnaires, preregistration, and other study materials are publicly available on our OSF page ([https://osf.io/xta67/](https://osf.io/xta67/)). Our annotated analysis scripts and results are publicly available on our study website ([https://jjcurtin.github.io/study_lag/](https://jjcurtin.github.io/study_lag/)).\n\n\n## Participants\n\nWe recruited 192 participants in early recovery from AUD in Madison, Wisconsin, USA for a 3-month longitudinal study. This sample size was determined based on traditional power analysis methods for logistic regression [@hsiehSampleSizeTables1989] because comparable approaches for machine learning models have not yet been validated. Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required that participants:   \n\n1.  were age 18 or older,\n2.  could write and read in English,\n3.  had at least moderate AUD (\\>= 4 self-reported DSM-5 symptoms), \n4.  were abstinent from alcohol for 1-8 weeks, and \n5.  were willing to use a single smartphone (personal or study provided) while on study.\n\nWe also excluded participants exhibiting severe symptoms of psychosis or paranoia (Defined as scores >2.2 or 2.8, respectively, on the psychosis or paranoia scales of the Symptom Checklist–90 [@derogatislBriefSymptomInventory])\n\nOf the 192 eligible participants, 191 consented to participate in the study at the screening visit, and 169 subsequently enrolled in the study at the enrollment visit, which occurred approximately one week later. Fifteen participants discontinued before the first monthly follow-up visit. We excluded data from one participant who did not maintain a goal of abstinence during their participation. We also excluded data from two participants due to evidence of careless responding and unusually low compliance. Our final sample consisted of 151 participants.  \n\n## Procedure\n\nParticipants completed five study visits over approximately three months. After an initial phone screen, participants attended an in-person screening visit to determine eligibility, complete informed consent, and collect self-report measures. Eligible, consented participants returned approximately one week later for an intake visit. Three additional follow-up visits occurred about every 30 days that participants remained on study. Participants were expected to complete four daily EMAs. Other personal sensing data streams (geolocation, cellular communications, sleep quality, and audio check-ins) were collected as part of the parent grant's aims (R01 AA024391). Participants could earn up to \\$150/month if they completed all study visits, had 10% or less missing EMA data, and opted in to provide data for other personal sensing data streams.\n\n## Ethics\nAll procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study #2015-0780) and carried out in accordance with the principles of the Declaration of Helsinki. All participants provided written informed consent.\n\n## Measures\n\n### Ecological Momentary Assessments\n\nParticipants completed four brief (7-10 questions) EMAs daily. The first and last EMAs of the day were scheduled within one hour of participants' typical wake and sleep times. The other two EMAs were scheduled randomly within the first and second halves of their typical day, with at least one hour between EMAs. Participants learned how to complete the EMA and reviewed the meaning of each question with a member of the research team during their intake visit to ensure consistent question interpretation.\n\nOn all EMAs, participants reported dates and times of any previously unreported past alcohol use. Next, participants rated the maximum intensity of recent (i.e., since last EMA) experiences of craving, risky situations, stressful events, and pleasant events. Finally, participants rated their current affect on two bipolar scales: valence (Unpleasant/Unhappy to Pleasant/Happy) and arousal (Calm/Sleepy to Aroused/Alert). On the first EMA each day, participants also rated anticipated risky situations, stressful events, and the likelihood that they would drink alcohol in the next week (i.e., abstinence self-efficacy).\n\n\n### Individual Characteristics\n\nWe collected self-report information about demographics (age, sex at birth, race, ethnicity, education, marital status, employment, and income) and clinical characteristics (AUD milestones, number of quit attempts, lifetime AUD treatment history, lifetime receipt of AUD medication, DSM-5 AUD symptom count, current drug use [@whoassistworkinggroupAlcoholSmokingSubstance2002], and presence of psychological symptoms [@derogatislBriefSymptomInventory] to characterize our sample. DSM-5 AUD symptom count and presence of psychological symptoms were also used to determine eligibility. Demographics were included as features in our models. A subset of these variables (sex at birth, race, ethnicity, and income) were used for model fairness analyses, as they have documented disparities in treatment access and outcomes. As part of the aims of the parent project, we collected many other trait and state measures throughout the study. A complete list of all measures can be found on our study's OSF page  ([https://osf.io/xta67/](https://osf.io/xta67/)). \n\n\n## Data Analytic Strategy\n\nData preprocessing, modeling, and Bayesian analyses were done in R (version 4.4.2) using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020; @kuhnTidyposteriorBayesianAnalysis2022; @goodrichRstanarmBayesianApplied2023]. Models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computing [@chtc].\n\n### Predictions\n\nA *prediction timepoint* (@fig-method, Panel A) is the hour at which our model calculates a predicted probability of a lapse within a future 24-hour prediction window for any specific individual. We calculated the features used to make predictions at each prediction timepoint within a feature scoring epoch that included all available EMAs up until, but not including, the prediction timepoint. The first prediction timepoint for each participant was 24 hours from midnight on their study start date. This ensured at least 24 hours of past EMAs were available in the feature scoring epoch. Subsequent prediction timepoints for each participant repeatedly rolled forward hour-by-hour until the end of their study participation.\n\nThe *prediction window* (@fig-method, Panel B) spans a period of time in which a lapse might occur. The prediction window width for all models was 24 hours (i.e., models predicted the probability of a lapse occurring within a specific 24-hour period). Prediction windows rolled forward hour-by-hour with the prediction timepoint. However, there were five possible *lag times* between the prediction timepoint and start of the associated prediction window. A prediction window either started immediately after the prediction time point (no lag) or was lagged by 1 day, 3 days, 1 week, or 2 weeks. \n\nGiven this structure, our models provided hour-by-hour predicted probabilities of an alcohol lapse in a future 24-hour period.  Depending on the model, that future period (the prediction window) might start immediately after the prediction timepoint or up to 2 weeks into the future.  For example, at midnight on the 30th day of participation, the feature scoring epoch would include the past 30 days of EMAs.  Separate models would predict the probability of lapse for 24-hour periods starting at midnight that day, or 24-hour periods starting 1 day, 3 days, 1 week or 2 weeks after midnight on day 30.  \n\n\n\n\n\n\n{{< embed notebooks/mak_figures.qmd#fig-method >}}\n\n\n\n\n\n\n\n\n\n### Labels\n\nThe start and end dates and times of past drinking episodes were reported on the first EMA item. A prediction window was labeled *lapse* if the start date/hour of any drinking episode fell within that window. A window was labeled *no lapse* if no alcohol use occurred within that window +/- 24 hours. If no alcohol use occurred within the window but did occur within 24 hours of the start or end of the window, the window was excluded. We used this conservative 24-hour fence for labeling windows as no lapse (vs. excluded) to increase the fidelity of these labels.  Given that most windows were labeled no lapse, and the outcome was highly unbalanced, it was not problematic to exclude some no lapse events to further increase confidence in those labels.\n\nThis method produced totals of: 274,179 labels for the baseline (no lag) model; 270,911 labels for the 1-day lagged model; 264,362 labels for the 3-day lagged model; 251,458 labels for the 1-week lagged model; and 228,420 labels for the 2-week lagged model.\n\n### Feature Engineering\n\nFeatures were calculated using only data collected in feature scoring epochs before each prediction timepoint to ensure our models were making true future predictions. For the no lag model, the prediction timepoint was at the start of prediction window, so all data prior to the start of the prediction window were included. For the lagged models, the prediction timepoint was 1 day, 3 days, 1 week, or 2 weeks prior to the start of the prediction window, so the last EMA data used for feature engineering were collected 1 day, 3 days, 1 week, or 2 weeks prior to the start of the prediction window. \n\nA total of 285 features were derived from three data sources:    \n\n1. *Prediction window*: We dummy-coded features for day of the week at the start of the prediction window.\n\n2. *Demographics*: We created quantitative features for age (in years) and personal income (in dollars), and dummy-coded features for sex at birth (male vs. female), race/ethnicity (non-Hispanic White vs. non-White and/or Hispanic), marital status (married vs. not married vs. other), education (high school or less vs. some college vs. college degree), and employment status (employed vs. unemployed). \n\n3. *Previous EMA responses*: We calculated raw and change features using EMAs in varying feature scoring epochs (i.e., 12, 24, 48, 72, and 168 hours) before the prediction timepoint for all EMA items. Raw features included min, max, and median scores for each EMA item across all EMAs in each epoch for a given participant. We calculated change features by subtracting each participant's baseline mean score for each EMA item from their raw feature.  These baseline mean scores were calculated using all of a participant's EMAs collected from the start of participation until the prediction timepoint. We also created raw and change features based on the most recent response for each EMA question and raw and change rate features from previously reported lapses and number of completed EMAs.   \n\n<!--JC to review paragraph below-->\nFeatures had missing values if the participant did not respond to the relevant EMA question during the associated scoring epoch. The proportion of missing values across features and models was low (median = .02, range = 0 - .13). We imputed missing data using median imputation for numeric features and mode imputation for nominal features. We selected coarse median/mode methods for handling missing data due to the computational costs associated with more advanced forms of imputation (e.g., KNN imputation, multiple imputation). Importantly, our imputation calculations are done using only held-in data and can be applied to any new observation.\n\nOther generic feature engineering steps included removing zero and near-zero variance features as determined from held-in data. A sample feature engineering script (i.e., tidymodels recipe) containing all feature engineering steps is available on our OSF study page. \n\n### Model Configurations\n\nWe trained and evaluated five separate classification models: one baseline (no lag) model and one model for 1-day, 3-day, 1-week, and 2-week lagged predictions. We considered four well-established statistical algorithms (elastic net, XGBoost, regularized discriminant analysis, and single layer neural networks) that vary across characteristics expected to affect model performance (e.g., flexibility, complexity, handling higher-order interactions natively) [@kuhnAppliedPredictiveModeling2018]. Candidate model configurations differed across sensible values for key hyperparameters. Configurations also differed on outcome resampling method (i.e., no resampling and up-sampling and down-sampling of the outcome using majority/no lapse to minority/lapse ratios ranging from 5:1 to 1:1).\n\n### Cross-validation\n\nWe used participant-grouped, nested cross-validation for model training, selection, and evaluation with auROC. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case (lapse) relative to a randomly selected negative case (no lapse). Grouped cross-validation assigns all data from a participant as either held-in or held-out to avoid bias introduced when predicting a participant's data from their own data. Folds were stratified on a between-subject variable of low vs. high lapsers (low lapsers reported fewer than 10 lapses while on study, and high lapsers reported 10 or more lapses while on study). We used 2 repeats of 5-fold cross-validation for the inner loops (i.e., *validation* sets) and 6 repeats of 5-fold cross-validation for the outer loop (i.e., *test* sets). Best model configurations were selected using median auROC across the 10 validation sets. Final performance evaluation of those best model configurations used median auROC across the 30 test sets.\n\n\n### Bayesian Model\n\nWe used a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) from the 30 held-out test sets for our five best models. Following recommendations from the rstanarm team and others [@rstudioteamRStudioIntegratedDevelopment2020; @gabryPriorDistributionsRstanarm2023], we used the rstanarm default autoscaled, weakly informative, data-dependent priors that take into account the order of magnitude of the variables to provide some regularization to stabilize computation and avoid over-fitting. Priors were set as follows: residual standard deviation ~ normal(location=0, scale=exp(2)), intercept (after centering predictors) ~ normal(location=2.3, scale=1.3), the two coefficients for window width contrasts ~ normal (location=0, scale=2.69), and covariance ~ decov(regularization=1, concentration=1, shape=1, scale=1). We set two random intercepts to account for our resampling method: one for the repeat, and another for the fold nested within repeat. We specified two sets of pre-registered contrasts for model comparisons. The first set compared each lagged model to the baseline no lag model (1-day lag vs. no lag, 3-day lag vs. no lag, 1-week lag vs. no lag, 2-week lag vs. no lag). The second set compared adjacently lagged models (3-day lag vs. 1-day lag, 1-week lag vs. 3-day lag, 2-week lag vs. 1-week lag). auROCs were transformed using the logit function and regressed as a function of model contrast. \n\nFrom the Bayesian model we obtained the posterior distribution (transformed back from logit) and Bayesian CIs for auROCs for all five models. To evaluate our models' overall performance, we report the median posterior probability for auROC and Bayesian CIs. This represents our best estimate for the magnitude of the auROC parameter for each model. If the CIs do not contain .5 (chance performance), this provides strong evidence (> .95 probability) that our model is capturing signal in the data.  \n\nWe then conducted Bayesian model comparisons using our two sets of contrasts - baseline and adjacent lags. For both model comparisons, we determined the probability that the models' performances differed systematically from each other. We also report the precise posterior probability for the difference in auROCs and the 95% Bayesian CIs. \n\n### Fairness Analyses\n\nUsing the same 30 held-out test sets, we calculated the median posterior probability and 95% Bayesian CI for auROC for each model separately by race/ethnicity (non-White and/or Hispanic vs. non-Hispanic White), income (below poverty line vs. above poverty line, and sex at birth (female vs. male). We used course race/ethnicity groupings due to the limited diversity of these demographics in our sample. The poverty cutoff was defined from the 2024 federal poverty line for the 48 contiguous United States. Participants at or below $15,060 annual income were categorized as below poverty. We conducted Bayesian group comparisons to assess the likelihood that each model performs differently by group. We summarize the differences in posterior probabilities for auROC across models. Individual Bayesian fairness contrasts for all five models are available in the supplement.\n\n### Model Characterization\n\nTo further characterize and understand our models, we used our inner resampling procedure (2 repeats of 5-fold cross validation grouped on participant and stratified by high/low lapsers) on the full data set to select a single best model configuration for each classification model (no lag, 1-day, 3-day, 1-week, and 2-week lag). The final configuration selected for each model represents the most reliable and robust configuration for deployment. We can better understand our final models by looking at the calibration of the predicted probabilities and the most important features contributing to those predictions.\n\n### Model Calibration\n\nThe best model configuration for each classification model was fit on the full data set. We fit this configuration using single 5-fold cross-validation. This method allowed us to obtain a single predicted probability for each observation, while still using separate data for model training and prediction. We calibrated our probabilities using Platt scaling [@plattProbabilisticOutputsSupport1999]. We calculated Brier scores to assess the accuracy of our raw and calibrated probabilities for the no lag and 2-week lagged models. Brier scores range from 0 (perfect accuracy) to 1 (perfect inaccuracy). A table of Brier scores for all five models is available in the supplement. We provide calibration plots for the no lag and 2-week lagged models (calibration plots for all five models are available in the supplement).\n\n### Global Feature Importance\n\nWe used the same single 5-fold cross-validation procedure to calculate raw Shapley values for observations in our held-out folds. Raw Shapley values index the importance of any feature (or set/category of features as described below) to any single prediction for a specific observation (i.e., for a specific 24-hour window for a specific participant), which indicates the \"local importance\" of that feature [@lundbergUnifiedApproachInterpreting2017].  More precisely, the magnitude of the raw Shapley value for any feature indicates how much the feature score for that observation adjusted the prediction (in log-odds units) for that observation relative to the mean prediction across all observations.  Positive Shapley values indicate that the feature score increased the prediction for that observation and negative values indicate that the feature score decreased the prediction.   \n\nRaw Shapley values are additive across features for an observation, with their sum across features equal to the total adjustment of the predicted value for that observation vs. the mean predicted value across all observations.  This property allows raw Shapley values to be added together across features within a category to index the importance of that feature category. We created feature categories by summing raw Shapley values for all features associated with specific EMA items.  In three instances, we combined features across two similar EMA items (i.e., past and anticipated risky situations, past and anticipated stressful events, and affective valence and arousal) to yield seven feature categories for distinct constructs assessed by the original 10 EMA items.  Specifically, we calculated Shapley values for past use, craving, affective state, past/anticipated risky situations, past/anticipated stressful events, past pleasant events, and abstinence self-efficacy.  \n\nShapley values can be aggregated across observations to describe the global importance of any feature (or feature category) across all predictions (i.e., for all 24-hour windows for all participants) in the dataset.  Global feature importance is calculated by averaging the absolute value of the Shapley values for a feature across all observations.  A large mean absolute Shapley value indicates that the feature makes big contributions to the predictions across the dataset. Global feature importance is a descriptive statistic that indicates the importance of the feature for predictions in a specific dataset, rather than a hypothetical population of observations. We provide a descriptive plot of the relative ranking of feature categories by their global feature importance for the no lag and 2-week lagged models. Global feature importance plots for all five models are available in the supplement. \n\n# Results\n\n## Demographic and Lapse Characteristics\n\n[@tbl-demohtml] provides a detailed breakdown of the demographic and clinical characteristics of our sample (N = 151).\n\n\n\n\n\n\n{{< embed notebooks/mak_tables.qmd#tbl-demohtml >}} \n\n\n\n\n\n\n\n\n\n\n## Model Evaluation\n@fig-pp presents the full posterior probability distributions for auROC for each model (no lag, 1-day, 3-day, 1-week, and 2-week lag). The median auROCs from these posterior distributions were 0.91 (no lag), 0.89 (1-day lag), 0.88 (3-day lag), 0.87 (1-week lag), and 0.85 (2-week lag). These values represent our best estimates for the magnitude of the auROC parameter for each model. The 95% Bayesian CI for the auROCs for these models were relatively narrow and did not contain 0.5: no lag [0.90-0.92], 1-day lag [0.88-0.90], 3-day lag [0.87-0.90], 1-week lag [0.85-0.88], 2-week lag [0.83-0.87]. \n\n\n\n\n\n\n{{< embed notebooks/mak_figures.qmd#fig-pp >}} \n\n\n\n\n\n\n\n\n\n\n## Model Comparisons\n\n@tbl-model presents the median difference in auROC, 95% Bayesian CI, and posterior probability that that the auROC difference was smaller than 0 for all baseline and adjacent lag contrasts. Median auROC differences less than 0 indicate the more lagged model, on average, performed worse than the more immediate model (e.g., 1-day lag -- no lag, 3-day lag -- 1-day lag). There was strong evidence (probabilities = 1) that the lagged models performed worse than the baseline (no lag) model, with average drops in auROC ranging from 0.02-0.06, and the previous adjacent lagged model, with average drops in auROC ranging from 0.01-0.02.\n\n\n\n\n\n{{< embed notebooks/mak_tables.qmd#tbl-model >}} \n\n\n\n\n\n\n\n\n## Fairness Analyses\n\n@tbl-fairness presents the median difference in auROC, 95% Bayesian CI, and posterior probability that the auROC difference was smaller than 0 for the three fairness contrasts: race/ethnicity (non-White and/or Hispanic; *N* = 20 vs. Non-Hispanic White; *N* = 131), sex at birth (female; *N* = 74 vs. male; *N* = 77), and income (below poverty line; *N* = 49 vs. above poverty line; *N* = 102). Median auROC differences less than 0 indicate the model, on average, performed worse for the non-advantaged group (female, non-White and/or Hispanic, below poverty line) compared to the advantaged group (male, non-Hispanic White, and above poverty line). In @tbl-fairness we present fairness analyses for our baseline model (no lag) and for our longest lagged model (2-week lag), as this is likely the most clinically useful lagged model for providing advanced warning of lapse risk. Fairness analyses for all five models are available in the supplement. \n\nThere was strong evidence (probabilities > .84) that our models performed worse for the non-advantaged groups compared to the advantaged groups. On average, across all five models, there was a median decrease in auROC of 0.13 (range 0.13-0.17) for participants who were non-White and/or Hispanic compared to participants who were non-Hispanic White. On average, across all five models, there was a median decrease in auROC of 0.05 (range 0.04-0.10) for female participants compared to male participants. On average, across all five models, there was a median decrease in auROC of 0.02 (range 0.01-0.04) for participants below the federal poverty line compared to participants above the federal poverty line. \n\nThe proportion of positive lapse labels over all labels (lapse and no lapse) for each demographic subgroup were relatively consistent across groups: race/ethnicity (6%, non-White and/or Hispanic vs. 8%, non-Hispanic White), income (12%, below poverty line vs. 7%, above poverty line), sex at birth (9%, female vs. 7%, male).\n\n\n\n\n\n\n{{< embed notebooks/mak_tables.qmd#tbl-fairness >}} \n\n\n\n\n\n\n\n\n## Model Calibration\n\nThe raw probabilities produced by our final models were not well calibrated. Consequently, we used Platt scaling to improve calibration. Platt scaling showed excellent improvement to the no lag model with a Brier score of .043. Calibration also improved probability accuracy for the 2-week lagged model with a Brier score of .063. For comparison, raw probability scores yielded Brier scores of .071 and .077 for the no lag and 2-week lagged models, respectively.\n\n@fig-cal shows the calibration plots for the raw and calibrated probabilities for the no lag and 2-week lagged model. It also includes a histogram of raw probabilities that demonstrates our models produced variable predicted probabilities, spanning nearly the entire 0 - 1 range. Calibration plots and Brier scores for all 5 models are available in the supplement.\n\n\n\n\n\n{{< embed notebooks/mak_figures.qmd#fig-cal >}} \n\n\n\n\n\n\n\n\n\n## Feature Importance\n\nGlobal feature importance is an indicator of how important a feature category was to the model's predictions, on average (i.e., across all participants and all observations). The top globally important feature category (i.e., highest mean |Shapley value|) for all models was past use. Future efficacy was a strong predictor for more immediate model predictions (i.e., no lag), but its importance diminished as lag time increased. On the other hand, as lag time increased, past/future risky situations increased in importance. Craving was consistently important in magnitude across all models. @fig-4 shows the relative ranking of feature categories for the no lag and 2-week lagged models. A plot of global feature importance for each feature category as a function of lag time is available in the supplement. These findings were also consistent across demographic subgroups (plots of global feature importance by demographic group are available for the no lag and 2-week lagged models in the supplement). \n\n\n\n\n\n\n{{< embed notebooks/mak_figures.qmd#fig-4 >}} \n\n\n\n\n\n\n\n\n\n# Discussion\n\n## Model Evaluation and Comparisons\n\nAll the models that we evaluated performed exceptionally well. The no lag model, which predicts the probability of an immediate (i.e., within 24 hours) lapse back to alcohol use, had a .91 median posterior probability for auROC. Our 2-week lagged model, which made the most distal predictions, had a .85 median posterior probability for auROC, suggesting lagged models can be used to shift a 24-hour prediction window meaningfully into the future. \n\nAcross models (no lag, 1 day, 3 days, 1 week, and 2 weeks), model performance systematically decreased as models predicted further into the future. All lagged models had lower performance compared to the no lag baseline model and to the preceding adjacent lag model. This is unsurprising given what we know about prediction and substance use. Many important relapse risk factors are fluctuating processes that can first emerge and/or change day-by-day, if not more frequently. As lag time increases, features become less proximal to the start of the prediction window. Still, we wish to emphasize that our lowest auROC (.85) is still quite good, and the benefit of advanced notice (i.e., 2 weeks) likely outweighs the modest cost to model performance.  \n\nCollectively, these results suggest we can achieve clinically meaningful performance up to two weeks out. Our rigorous resampling methods (grouped, nested, k-fold cross-validation) make us confident that these are valid estimates of how our models would perform with new individuals. Furthermore, it should be noted that both the no lag and 2-week lagged models can be combined in a complementary fashion that allows both for highly accurate immediate lapse prediction and advanced warning about future lapse risk.\n\n## Model Fairness\n\nIn recent years, the machine learning field has begun to recognize the need to evaluate model fairness when algorithms are used to inform important decisions (e.g., healthcare services offered, eligibility for loans, early parole). Algorithms that perform favorably for only majority group members may exacerbate existing disparities in access to resources and important clinical outcomes [@veinotGoodIntentionsAre2018]. In this study, we assessed model fairness by comparing model performance across important subgroups with known disparities in substance use treatment access and/or outcomes - race/ethnicity (non-White and/or Hispanic vs. non-Hispanic White), income (below poverty line vs. above poverty line), and sex at birth (female vs. male). \n\n<!-- JC review paragraph below-->\nAll models performed worse for people who were non-White and/or Hispanic, and for people who had an income below the poverty line. These findings should be interpreted cautiously given the lack of diversity in our training data. Participants of color were severely underrepresented in our training data (*N* = 20, 13%). Individuals below the poverty line were also underrepresented, though to a lesser degree (*N* = 49, 32%). Nevertheless, these findings are concerning and the fact that we do not have adequate sample sizes for these comparisons highlight a large limitation of our study.  \n\nAn obvious solution to this problem involves intentional recruitment for diversity in training data when developing prediction models.  For example, we are now working to increase the racial, ethnic, and income diversity of our training data for alcohol lapse prediction while simultaneously optimizing feedback from these models for implementation purposes [@wyantMaximizingEngagementTrustunderreview]. In a separate project, we developed a national recruitment method that enabled us to recruit for racial, ethnic and income diversity while also focusing on much needed diversity across geographic location (e.g., rural vs. urban; [@moshontzProspectivePredictionLapses2021]).  We expect geographic diversity in the training data may also be crucial to develop fair models because the features that predict lapse in urban and suburban settings may differ from those that predict lapse in rural environments. If rural participants are not used to train models, the implementation of these models may compound existing disparities in SUD treatment in these communities [@leeUrbanRuralDisparities2023; @listerSystematicReviewRuralspecific2020].  \n\nFuture research can also explore potential computational solutions to mitigate performance disparities that emerge when subgroups are poorly represented in available training data.  For example, training data from under-represented subgroups could be up-sampled (e.g., using the synthetic minority oversampling technique), or the cost functions used by the learning algorithms could be adjusted to differentially weigh prediction errors based on participant characteristics.  In another vein, modeling approaches that yield idiographic, person-specific models [@fisherDynamicModelPsychological2015; @davidIntraindividualDynamicNetwork2018; @rocheEnrichingPsychologicalAssessment2014; @wrightModelingHeterogeneityMomentary2016] may reduce performance disparities across subgroups.  For example, we have begun to develop state space models whose parameters can be initialized with priors derived from existing training data but then adjusted over time to fit patterns present within a specific individual's time-series [@pulickIdiographicLapsePrediction2025].  Such models may mitigate issues of unfairness to a large degree because they will weigh the individual's own data more heavily than group level estimates over time as more data accrue.  \n\nOf note, problems with model fairness can emerge even when subgroups are well-represented in the training data. Our models performed less well for women compared to men despite the fact that women were well-represented in the training data (*N* = 74, 49%).  Instead, this differential performance may have resulted from more fundamental problems with the features available to the model.  We chose our EMA items using domain expertise from decades of research on the factors that predict relapse. However, prior to the 1993 National Institute of Health Revitalization Act [@studiesNIHRevitalizationAct1994] that mandated the inclusion of minorities and women in research, women were mostly excluded from substance use treatment research due to their childbearing potential [@vannicelliEffectSexBias1984]. As a result, it is possible that our theories about the causes and contributors to relapse are biased toward constructs that are more relevant for men than women.  If true, features derived from EMA items that tap these constructs would be expected to under-perform when predicting lapses for women.  More research may be needed to identify relapse risk factors for women (e.g., interpersonal relationship problems [@walitzerGenderDifferencesAlcohol2006], hormonal changes [@mchughSexGenderDifferences2018]), and other groups under-represented in the literature before we can fully address these performance disparities.  \n\nIn the meantime, data-driven (bottom-up) approaches can be used to engineer high-dimensional feature sets that are not explicitly grounded in existing, and potentially biased, theories.  For example, we have begun to explore the application of natural language processing techniques (e.g., LIWC; topic modeling; BERT [@tausczikPsychologicalMeaningWords2010; @bleiLatentDirichletAllocation2003; @devlinBERTPretrainingDeep2019]) to text messages and other social media activity by our participants to engineer features that may predict future lapses. Such features may or may not align with existing theories about relapse, but because they are anchored to participants' own words, they may serve as reliable indicators of lapse risk for certain individuals, particularly when used within learning algorithms that employ feature selection, regularization, or other techniques to address the bias-variance trade-off with high-dimensional feature sets. Furthermore, emerging techniques for interpreting machine learning models [@molnarInterpretableMachineLearning2022] can be applied to models that perform well to bootstrap the identification of new lapse risk constructs based on these novel features. \n\nBeyond issues of training data representation and lacunae or outright biases in our theories, it is also true that historically marginalized groups that have experienced systemic racism, exclusion, or other stigma around substance use (e.g., societal expectations for women regarding attractiveness, cleanliness and motherhood [@meyersIntersectionGenderDrug2021]) may feel less trusting in disclosing substance use [@marwickPrivacyMarginsUnderstanding2018]. These experiences could prompt some individuals in these subgroups to under-report lapses and/or risk factors, which could also degrade performance and evaluation of our models for these subgroups.  We observed relatively comparable percentages of lapses reported among disadvantaged compared to advantaged groups. However, comparable lapse rates do not necessarily confirm comparable reporting accuracy because it is possible that there were systematic differences in lapse rates across groups that were masked by issues of trust.  \n\n## Model Characterization\n\n### Calibration\n\nAfter applying Platt scaling to our predicted probabilities, our models were generally well calibrated with increasing monotonic relationships between calibrated model output and lapse event rates. Well-calibrated probabilities indicate that the predicted probability aligns closely with the true likelihood of an outcome (i.e., a lapse). Our no lag model had excellent calibration. However, the calibration plots suggest that with a longer lag time of 2 weeks, the model tends to over-predict the likelihood of lapses when predicted probabilities were higher. \n\nThis pattern may not necessarily be problematic. Research suggests that people often struggle to interpret probabilistic feedback, especially when it's provided in raw numerical form [@zikmund-fisherRightToolWhat2013; @fagerlinMakingNumbersMatter2007; @zipkinEvidencebasedRiskCommunication2014]. As a result, it may be more effective to communicate risk using coarser categories (e.g., low, medium, or high risk) or through relative changes in risk (e.g., “Your risk of lapse is higher this week compared to last week”). These forms of feedback may be less sensitive to small miscalibrations at the extremes as long as the relationship between predicted probabilities and the observed event rate is monotonic. \n\n### Feature importance\n\nThe relative ordering of top global features remained somewhat consistent across the no lag and 2-week lagged models. Past use was the most important feature in both models in our dataset. This is not surprising given that our outcome was lapse, and past behavior is often the best predictor of future behavior. This finding also supports decades of clinical research on relapse prevention, where lapses (i.e., single instances of goal inconsistent alcohol use) are seen as powerful precursors to relapse (i.e., full return to harmful drinking; [@marlattRelapsePreventionMaintenance1985]). Abstinence self-efficacy emerged as the second most important feature in both models in our dataset, indicating that participants had reasonably accurate insight into their near-term success with maintaining alcohol abstinence. Craving was also an important predictor in both models, suggesting that it may be an important target for intervention to support early recovery efforts.\n\nSeveral feature categories displayed sizeable differences in global importance by lag time.  The importance of abstinence self-efficacy dropped by more than 50% in the 2-week lagged model relative to the no lag model.  This may indicate that self-efficacy during early recovery is unstable even across shorter periods of time such that their current self-efficacy does not strongly predict abstinence success even two weeks into the future. In fact, craving and risky situations become as important as self-efficacy when predicting two-week lagged lapses. It may be that these other experiences are shaping and changing the individual's self-efficacy rapidly in early recovery.  This also suggests that more frequent clinical assessments of self-efficacy as a target for intervention may be needed rather than assuming stability in this construct if initial assessment suggests it is high.  Also, our study cannot determine if this differential importance of self-efficacy for immediate vs. lagged lapses persists beyond early recovery (where people may be encouraged to take a \"one day at a time\" mindset).  Self-efficacy may become a more stable predictor of future abstinence success after longer periods of recovery, but our sample was limited to participants in early recovery (<= 8 weeks of abstinence at intake).    \n\nPast use was less important for the 2-week lagged model compared to the no lag model. This indicates that the predictive strength of a lapse on the likelihood of subsequent lapses diminishes to some degree over a relatively short period of time. This is good news and reinforces that single lapses do not always mark a return to consistent patterns of frequent, and potentially harmful, alcohol use.   Despite this reduction in importance of past use as a predictor of lagged alcohol use, past use did remain the most important category for two-week lagged lapses. Lapses may provide \"teachable moments\" that can be used to reinforce recovery motivation, better understand risks, and develop skills to address those risks [@witkiewitzModelingComplexityPosttreatment2007]. Conversely, lapses should not be ignored because they remain strong predictors of further use.\n\nSurprisingly, past and anticipated risky situations were more important in the 2-week lagged vs. no lag model, suggesting that the impact of these situations on lapses back to use may be delayed.  It may be that persistent exposure to risks is necessary to undermine an abstinence goal and lead to return to alcohol use.  Alternatively or additionally, people may also be better able to anticipate future risky situations (e.g., vacations, anniversaries of significant dates) than future acute stressors or even future self-efficacy.  Regardless, the increased importance of risky situations for predicting lagged lapses provides an opportunity to intervene prior to the lapse, particularly if the individual is encouraged to assess future risks and/or makes use of a recovery monitoring prediction model. \n\nWe were also surprised that stressful events, pleasant events, and affective state features did not make more important contributions to predictions across models. These constructs are highlighted in numerous theories about addiction and relapse [@marlattRelapsePreventionMaintenance1985; @witkiewitzRelapsePreventionAlcohol2004; @rawsonIntensiveOutpatientApproach1995] and represent targets for intervention in many existing treatments [@mchughCognitiveBehavioralTherapySubstance2010; @lieseCognitiveBehavioralTherapyAddictive2022; @bowenMindfulnessBasedRelapsePrevention2021; @centerforsubstanceabusetreatmentCounselorsTreatmentManual2006].  It may be that their impact is subsumed within other more powerful features (i.e., past use and self-efficacy).  However, this seems unlikely given that the methodology underlying Shapley values allows for a fair distribution of importance among the relevant predictive features even when those features are correlated [@molnarInterpretableMachineLearning2022].  Alternatively, we may need to more carefully consider the nuanced roles that these constructs play (e.g., within the context of individual coping strategies, social support or environmental factors) in the return to alcohol use during recovery [@fronkStressAllostasisSubstance2020].\n\n## Additional Limitations and Future Directions\n\nWe believe our lapse prediction models will be most effective when embedded in an RMSS designed to deliver adaptive and personalized continuing care. This system could send daily, weekly, or less frequent messages to users with personalized feedback about their risk of lapse and provide support recommendations tailored to their current recovery needs. This study provides initial support that immediate and lagged prediction models can be trained to high accuracy using EMA for recovery monitoring. Furthermore, locally important features from these models can be used to identify the specific factors that contribute to each lapse risk prediction.  \n\nThe no lag model can be used to guide individuals to take immediate, actionable steps to maintain their recovery goals and support them to implement these steps within the RMSS. For example, the RMSS can recommend an embedded urge surfing activity when someone's immediate risk is driven by strong craving whereas a guided relaxation video can be provided to the user when they report stressful events.  Similarly, the RMSS can encourage (and explicitly support) the user to reflect on recent past successes and/or skills they have developed when their self-efficacy is low.\n\nThe 2-week lagged model provides individuals with advanced warning of their lapse risk. This model is well-suited to support recovery needs that cannot be addressed immediately within an RMSS app, such as scheduling positive or pleasant activities, increasing social engagement, or attending a peer-led recovery meeting. To be clear, we do not believe an RMSS app alone will be sufficient to deliver continuing care. We expect individuals will require additional support throughout their recovery from a mental health provider (e.g., motivational enhancement, crisis management, skill building), a peer (e.g., sponsor, support group), or family member. Importantly, these types of supports take time to set up, highlighting the value of this lagged 2-week model.\n\nAt this point, it is still unclear the best way to provide risk and support information from our models to people. For an RMSS to be successful, users must trust the system, consistently engage with the system over time, and find the system beneficial. We have recently launched an NIAAA funded project to optimize daily support messages by examining the impact of several key message components (e.g., lapse probability, locally important features, a risk-relevant recovery activity recommendation, the linguistic style and tone of the message) on engagement, trust, clinical outcomes [@wyantMaximizingEngagementTrustunderreview].  \n\nFor a system using lagged models, we can imagine that lags longer than two weeks (i.e., more advanced warning) would be better still. In the present study, we could not train models with lags longer than two weeks because participants only provided lapse reports for up to three months. With the 2-week lagged model, we had approximately 17% fewer labeled observations for training because the first two weeks (out of 12 weeks) of labels for each participant were discarded. This data loss may be one factor that contributed to the decreases in model performance with increases in lag time and we believed that greater data loss (e.g., 25% for a 3-week lag) would not be tenable. We have recently completed data collection on a NIDA funded project where participants provided EMA and other sensed data for up to 12 months [@moshontzProspectivePredictionLapses2021]. These data will allow us to train models with longer lags and to better evaluate the impact of data loss on model performance because lag time can be increased substantially with proportionally less data loss given 52 weeks of labeled observations per participant. \n\n<!--JC to review paragraph below-->\nWhile the number of observations are large and support the complexity of a machine learning pipeline, we are limited by the number of participants. Successful machine learning models must generalize well to new data. Our analyses only speak to how well our models generalize to a mostly homogenous sample of 151 participants. We made the most efficient use of our sample (i.e., using nested cross-validation to maximize the amount of data used for both selection and evaluation); however, there is reason to worry about the generalizability to other populations, such as individuals living in rural locations and members of other diverse groups not adequately represented in our training data.   \n\nAdditionally, our use of features from 4x daily EMA as model inputs may raise concerns about measurement burden. We confirmed that participants can comply with such EMA schedules over this time period and that they find it acceptable given its potential benefits to them [@wyantAcceptabilityPersonalSensing2023; see also @jonesComplianceEcologicalMomentary2019]. However, frequent daily surveys may become too burdensome within an RMSS intended for use over many, many months to years for long-term continuing care.  We have begun to address this concern by training no lag models with fewer EMAs (1x daily) and have found comparable performance [@pulickIdiographicLapsePrediction2025]. Additionally, reinforcement learning could potentially be used for adaptive EMA sampling. For example, each day the algorithm could make a decision to send out an EMA or not based on inferred latent states of the individual based on previous EMA responses and predicted probability of lapse.\n\nWe have also begun to explore how we can supplement our models with data from lower burden sensing methods. Geolocation, which can be passively sensed, could compliment EMA well [@baeLeveragingMobilePhone2023]. First, it could provide insight into information not easily captured by self-report without lengthy surveys. For example, the amount of time spent in risky locations, or changes in routine (e.g., loss of job; move to new city) that could indicate life stressors can be detected in movement patterns. Second, the near-continuous sampling of geolocation could offer risk-relevant information that would otherwise be missed in between the discrete sampling periods of EMA. Furthermore, potentially powerful features can be engineered by combining geolocation data with contextual information available in public sources (e.g., census data, alcohol outlet density) [@huangActivityIdentificationGPS2010; @xieTrajectoriesActivitiesSpatiotemporal2009] or collected from the user directly (e.g., self-evaluated riskiness of a given location) [@moshontzProspectivePredictionLapses2021].\n\n\n\n## Conclusions\n\nThis study suggests it is possible to accurately predict alcohol lapses both immediately and up to two weeks into the future using lagged machine learning prediction models. The no lag model could guide users to engage with a smart RMSS that provides daily recovery activities that are personalized to their lapse risk and the factors contributing to that risk. The 2-week lagged model could enable patients to seek out and implement recovery support that is not immediately available to them within the RMSS. Several important steps remain prior to implementing the no lag and 2-week lagged models within a smart RMSS.  Feedback and support messages from these models should be optimized to sustain system engagement, trust, and clinical outcomes.  Passive sensing of model inputs may allow assessment of a broader range of risk factors with less burden for system users.  Perhaps most important, model fairness must be improved by decreasing disparities in performance for less privileged groups.  We remain optimistic about the potential to implement these models within a smart RMSS because these barriers, while challenging, are surmountable.\n\n## Acknowledgments\nThis research was supported by grants from the National Institute on Alcohol Abuse and Alcoholism (NIAAA; R01 AA024391 to John J. Curtin) and the National Institute on Drug Abuse (NIDA; R01 DA047315 to John J. Curtin). The authors wish to thank Susan E. Wanta for her role as the project administrator.\n\n\n# References\n::: {#refs}\n:::\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}