---
title: "Beta callibration notes"
author: "Kendra Wyant"
format: html
editor_options: 
  chunk_output_type: console
---

## Beta Callibration

### Problem

Error getting with 2-week lag model: `f() values at end points not of opposite sign`.  


Information on this issue below. Still unclear why this error is not occurring with 0 lag model. Also not every outer split in 2-week model gets this error! *Check if getting error with any other lags.*


### Background

This error usually comes from the beta calibration fitting procedure failing during the optimization step. Specifically, it typically arises during root-finding or optimization when the method being used expects the function values at the endpoints of the interval to have opposite signs, but they don't.

This is often seen when:

- The predicted probabilities are too close to 0 or 1 (not suitable for beta calibration).
- The optimization interval is not well-specified.
- The data doesn't contain a strong enough relationship to support fitting the beta calibration model.

Beta calibration fits a logit-transformed beta distribution to your predicted probabilities. It's a more flexible method than Platt (logistic) scaling or isotonic regression, but it also assumes that the input probabilities are well-calibrated to begin with (i.e., not all close to 0 or 1). *Could this be a problem because of many probabilities close to 0?*

Side note on other calibration methods:  

- Platt scaling (logistic regression on the logit of predicted probs)
- Isotonic regression (non-parametric, good for monotonic but non-linear corrections)

Effectiveness of different calibrations in 0 lag model - *Beta clearly seems to be the best*  

**Platt Scaling (logistic)**
```{r}
#| code-fold: true
#| message: false
#| warning: false

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(probably))
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")
theme_set(theme_classic()) 
path_models_lag <- format_path(str_c("studydata/risk/models/lag"))
path_chtc <- format_path(str_c("studydata/risk/chtc/lag"))
probs_0 <- read_rds(here::here(path_models_lag, "outer_preds_6_x_5_1day_0_v3_nested_strat_lh.rds"))
probs_336 <- read_rds(here::here(path_models_lag, "outer_preds_6_x_5_1day_336_v3_nested_strat_lh.rds"))

bin_width = 0.10

probs_0 |> 
  mutate(bins = cut(prob_logi, breaks = seq(0, 1, bin_width)), 
         lapse = if_else(label == "Lapse", 1, 0)) |> 
  group_by(bins)  |> 
  summarize(mean_lapse = mean(lapse),
            .groups = "drop") |>
  mutate(bins = as.numeric(bins),
         midpoints = bin_width/2 + bin_width * (bins - 1))  |> 
  ggplot(data = _, aes(x = midpoints, y = mean_lapse)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
  geom_line() +
  geom_point() +
  labs(x = "Predicted Lapse Probability (bin mid-point)",
       y = "Observed Lapse Probability") +
  scale_x_continuous(breaks = seq(0, 1, bin_width),
                     limits = c(0, 1)) +
  scale_y_continuous(breaks = seq(0, 1, bin_width),
                     limits = c(0, 1)) 
```

**Beta Callibration**
```{r}
#| code-fold: true

probs_0 |> 
  mutate(bins = cut(prob_beta, breaks = seq(0, 1, bin_width)), 
         lapse = if_else(label == "Lapse", 1, 0)) |> 
  group_by(bins)  |> 
  summarize(mean_lapse = mean(lapse),
            .groups = "drop") |>
  mutate(bins = as.numeric(bins),
         midpoints = bin_width/2 + bin_width * (bins - 1))  |> 
  ggplot(data = _, aes(x = midpoints, y = mean_lapse)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
  geom_line() +
  geom_point() +
  labs(x = "Predicted Lapse Probability (bin mid-point)",
       y = "Observed Lapse Probability") +
  scale_x_continuous(breaks = seq(0, 1, bin_width),
                     limits = c(0, 1)) +
  scale_y_continuous(breaks = seq(0, 1, bin_width),
                     limits = c(0, 1)) 
```

**Isotonic Regression**  
```{r}
#| code-fold: true
#| warning: false

probs_0 |> 
  mutate(bins = cut(prob_iso, breaks = seq(0, 1, bin_width)), 
         lapse = if_else(label == "Lapse", 1, 0)) |> 
  group_by(bins)  |> 
  summarize(mean_lapse = mean(lapse),
            .groups = "drop") |>
  mutate(bins = as.numeric(bins),
         midpoints = bin_width/2 + bin_width * (bins - 1))  |> 
  ggplot(data = _, aes(x = midpoints, y = mean_lapse)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
  geom_line() +
  geom_point() +
  labs(x = "Predicted Lapse Probability (bin mid-point)",
       y = "Observed Lapse Probability") +
  scale_x_continuous(breaks = seq(0, 1, bin_width),
                     limits = c(0, 1)) +
  scale_y_continuous(breaks = seq(0, 1, bin_width),
                     limits = c(0, 1)) 
```


### Trouble Shooting

* look at performance in folds
* code to pull examples of folds that work 
* pull all folds that don't work
* characterize folds (num lapses, performance, range of probabilities/distribution)

Pull out predicted probabilities for a problematic fold (outer split 2)
```{r}
probs_336_2 <- probs_336 |> 
  filter(outer_split_num == 2)
```

Check distribution of probabilities
```{r}
probs_336_2 |> 
  ggplot(aes(x = prob_raw)) +
  geom_histogram(color = "black", fill = "light grey")
```

Does this look different then split 3 that returns no error?
```{r}
probs_336 |> 
  filter(outer_split_num == 3) |> 
  ggplot(aes(x = prob_raw)) +
  geom_histogram(color = "black", fill = "light grey")
```

Or split 1?
```{r}
probs_336 |> 
  filter(outer_split_num == 1) |> 
  ggplot(aes(x = prob_raw)) +
  geom_histogram(color = "black", fill = "light grey")
```


Odd truncated distribution in split 1 and 3 - *May explain weird peaks seeing in histogram of all probabilities* - checked and 14/30 splits are like this


Beta callibration may not work well when there are sharp spikes at 0 or 1.

- Beta calibration models the relationship between predicted probabilities and actual outcomes by fitting a logistic function to the CDF of a Beta distribution. The beta distribution is defined on the open interval (0, 1) it cannot handle exact values of 0 or 1.
- The logit function becomes unstable at 0 or 1.
- The optimization routines used often rely on the function having different signs at the endpoints. When all probabilities are near 0 or 1, the function becomes flat or undefined, leading to the error `f() values at end points not of opposite sign`.

Potential solution is to "clip the probabilities slightly inward" (e.g., `probs <- pmin(pmax(predicted_probs, 1e-6), 1 - 1e-6)`)

There are no exact 0s or 1s though
```{r}
min(probs_336_2$prob_raw)
max(probs_336_2$prob_raw)
```

This is likely not caused by probability distribution, instead it might be a "numerical or optimization issue".


#### Reproduce error    

Get config, load data and make splits using training controls
```{r}
#| code-fold: true

metrics_raw <- 
  read_csv(here::here(path_models_lag, "inner_metrics_1day_336_v3_nested_strat_lh.csv"),
                                             col_types = "iiiiccdddcdddddddi")

metrics_avg <- metrics_raw |> 
  filter(algorithm == "xgboost") |> 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample, outer_split_num) |> 
   summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                     median),
              n_jobs = n(), .groups = "drop") |> 
  relocate(outer_split_num, n_jobs) |> 
  arrange(outer_split_num, desc(roc_auc))

configs_best <- metrics_avg |> 
  group_by(outer_split_num) |> 
  arrange(desc(roc_auc)) |> 
  slice(1) |> 
  ungroup() |> 
  relocate(roc_auc, .before = accuracy) 

path_batch <- here::here(path_chtc, "train_xgboost_1day_336lag_nested_2_x_5_6_x_5_v3_strat_lh")
source(here::here(path_batch, "input", "training_controls.R"))
source(here::here(path_batch, "input", "fun_chtc.R"))

d <- read_csv(here::here(path_batch, "input/data_trn.csv"), show_col_types = FALSE) 
lapse_strat <- read_csv(here::here(path_batch, "input/lapse_strat.csv"),
                            show_col_types = FALSE)

d <- format_data(d, lapse_strat) |> 
    arrange(label_num) |> 
    mutate(id_obs = 1:nrow(d))  # tmp add for linking obs
  
splits <- d |> 
    make_splits(cv_resample_type, cv_resample, cv_outer_resample, 
              cv_inner_resample, cv_group, cv_strat = cv_strat,
              the_seed = seed_splits)
```

Pull out split 2
```{r}
split_num <- 2

d_in <- training(splits$splits[[split_num]]) |> 
    select(-id_obs)  # not used for training; only needed in d_out to tag for later joins 
d_out <- testing(splits$splits[[split_num]])
  
config_best <- configs_best |> 
  slice(split_num) |> 
  rename(n_jobs_in = n_jobs, accuracy_in = accuracy, 
         bal_accuracy_in = bal_accuracy,
         roc_auc_in = roc_auc, sens_in =  sens, spec_in = spec, 
         ppv_in = ppv, npv_in = npv)
```


Refit model
```{r}
rec <- build_recipe(d = d_in, config = config_best)

rec_prepped <- rec |> 
  prep(training = d_in, strings_as_factors = FALSE)
  
feat_in <- rec_prepped |> 
  bake(new_data = NULL)
  
model_best <- fit_best_model(config_best, feat = feat_in, "classification")
  
feat_out <- rec_prepped |> 
  bake(new_data = d_out)   # no id_obs because not included in d_in
    
preds_prob <- predict(model_best, feat_out,
                      type = "prob")
```


Run Calibration

```{r}
# train calibration model train/test split on held in data
set.seed(2468)
cal_split <- d_in |> 
  group_initial_split(group = all_of(cv_group), prop = 3/4)
d_cal_in <- training(cal_split) 
d_cal_out <- testing(cal_split)
  
feat_cal_in <- rec |> 
  prep(training = d_cal_in, strings_as_factors = FALSE) |> 
  bake(new_data = NULL) 
  
feat_cal_out <- rec |> 
  prep(training = d_cal_in, strings_as_factors = FALSE) |> 
  bake(new_data = d_cal_out) 

model_cal <- fit_best_model(config_best, feat = feat_cal_in, "classification")
```

```{r}
#| eval: false
  
# beta calibration
beta <- predict(model_cal, feat_cal_out,
                type = "prob") |>
  mutate(truth = feat_cal_out$y) 

beta <- beta |>
  cal_estimate_beta(truth = truth,
                    estimate = dplyr::starts_with(".pred_"),
                    smooth = TRUE)
preds_prob_beta <- preds_prob |>
  cal_apply(beta)
  
# get beta calibrated probabilities
preds_prob_beta[[str_c(".pred_", y_level_pos)]]
```

`Error in uniroot(function(mh) b * log(1 - mh) - a * log(mh) - inter, c(1e-16,  :` 
`f() values at end points not of opposite sign`


### Working solution

Formula is same formula from betacal package. 

```{r}
library(betacal)

beta <- predict(model_cal, feat_cal_out,
                type = "prob") |>
  
  mutate(truth = feat_cal_out$y) 

cal_model <- betacal::beta_calibration(beta$.pred_yes, beta$truth, parameters = "abm")

# Extract parameters
a <- cal_model$map[1]
b <- cal_model$map[2]
m <- cal_model$map[3]

# Apply calibration formula
logit_calibrated <- a * log(preds_prob$.pred_yes) + b * log(1 - preds_prob$.pred_yes) + m
beta_d_out <- 1 / (1 + exp(-logit_calibrated))
```

*Need Yes as second factor level?*
```{r}
hist(beta_d_out)
```


### Checks

Calibrate fold 3 probabilities and compare to probabilities calibrated old way 

Run new way

New function 
```{r}
#| code-fold: true

fit_predict_eval <- function(split_num, splits, configs_best, 
                             calibration = TRUE){

  
  d_in <- training(splits$splits[[split_num]]) |> 
    select(-id_obs)  # not used for training; only needed in d_out to tag for later joins 
  d_out <- testing(splits$splits[[split_num]])
  
  config_best <- configs_best |> 
    slice(split_num) |> 
    rename(n_jobs_in = n_jobs, accuracy_in = accuracy, 
           bal_accuracy_in = bal_accuracy,
           roc_auc_in = roc_auc, sens_in =  sens, spec_in = spec, 
           ppv_in = ppv, npv_in = npv)
    
  rec <- build_recipe(d = d_in, config = config_best)
  rec_prepped <- rec |> 
    prep(training = d_in, strings_as_factors = FALSE)
  
  feat_in <- rec_prepped |> 
    bake(new_data = NULL)
  
  model_best <- fit_best_model(config_best, feat = feat_in, "classification")
  
  feat_out <- rec_prepped |> 
    bake(new_data = d_out)   # no id_obs because not included in d_in
    
  preds_prob <- predict(model_best, feat_out,
                        type = "prob")

  # train calibration model train/test split on held in data
  set.seed(2468)
  cal_split <- d_in |> 
    group_initial_split(group = all_of(cv_group), prop = 3/4)
  d_cal_in <- training(cal_split) 
  d_cal_out <- testing(cal_split)
  
  feat_cal_in <- rec |> 
    prep(training = d_cal_in, strings_as_factors = FALSE) |> 
    bake(new_data = NULL) 
  
  feat_cal_out <- rec |> 
    prep(training = d_cal_in, strings_as_factors = FALSE) |> 
    bake(new_data = d_cal_out) 

  model_cal <- fit_best_model(config_best, feat = feat_cal_in, "classification")
  
  # iso calibration
  iso <- predict(model_cal, feat_cal_out,
                 type = "prob") |> 
    mutate(truth = feat_cal_out$y) |> 
    cal_estimate_isotonic(truth = truth,
                            estimate = dplyr::starts_with(".pred_"))
  preds_prob_iso <- preds_prob |> 
    cal_apply(iso)
  
  # logistic calibration
  logi <- predict(model_cal, feat_cal_out,
                  type = "prob") |>
    mutate(truth = feat_cal_out$y) |>
    cal_estimate_logistic(truth = truth,
                          estimate = dplyr::starts_with(".pred_"),
                             smooth = TRUE)
  preds_prob_logi <- preds_prob |>
    cal_apply(logi)

  # beta calibration
  # beta <- predict(model_cal, feat_cal_out,
  #                 type = "prob") |>
  #   mutate(truth = feat_cal_out$y) |>
  #   cal_estimate_beta(truth = truth,
  #                     estimate = dplyr::starts_with(".pred_"),
  #                     smooth = TRUE)
  # preds_prob_beta <- preds_prob |>
  #   cal_apply(beta)
  
  beta <- predict(model_cal, feat_cal_out,
                type = "prob") |>
  mutate(truth = feat_cal_out$y) 

  cal_model <- betacal::beta_calibration(beta$.pred_yes, feat_cal_out$y, parameters = "abm")

    # Extract parameters
    a <- cal_model$map[1]
    b <- cal_model$map[2]
    m <- cal_model$map[3]

    # Apply calibration formula
    logit_calibrated <- a * log(preds_prob$.pred_yes) + b * log(1 - preds_prob$.pred_yes) + m
    beta_d_out <- 1 / (1 + exp(-logit_calibrated))
    
    # combine raw and calibrated probs
    probs_out <- tibble(id_obs = d_out$id_obs,
                        outer_split_num = rep(split_num, nrow(preds_prob)),
                        prob_raw = preds_prob[[str_c(".pred_", y_level_pos)]],
                        prob_iso = preds_prob_iso[[str_c(".pred_", y_level_pos)]],
                        prob_logi = preds_prob_logi[[str_c(".pred_", y_level_pos)]],
                        prob_beta = beta_d_out,
                        # prob_beta = preds_prob_beta[[str_c(".pred_", y_level_pos)]],
                        label = d_out$y) |> 
        mutate(label = fct_recode(label, "No lapse" = "no",
                                  "Lapse" = "yes"))

 # SHAP in held out fold
  # shaps_out <- SHAPforxgboost::shap.prep(xgb_model = extract_fit_engine(model_best),
  #                    X_train = feat_out |> select(-y) |>  as.matrix()) |> 
  #   # add id_obs by multiple of number of features
  #   mutate(id_obs = rep(d_out$id_obs, times = ncol(feat_out) - 1),
  #          split_num = split_num) |>  
  #   relocate(id_obs, split_num)
  
  return(list(probs_out = probs_out))
               
             # shaps_out = shaps_out))
}
```

```{r}
preds_1_new <- fit_predict_eval(1, splits, configs_best)
```

Old function
```{r}
#| code-fold: true

fit_predict_eval <- function(split_num, splits, configs_best, 
                             calibration = TRUE){
  
  d_in <- training(splits$splits[[split_num]]) |> 
    select(-id_obs)  # not used for training; only needed in d_out to tag for later joins 
  d_out <- testing(splits$splits[[split_num]])
  
  config_best <- configs_best |> 
    slice(split_num) |> 
    rename(n_jobs_in = n_jobs, accuracy_in = accuracy, 
           bal_accuracy_in = bal_accuracy,
           roc_auc_in = roc_auc, sens_in =  sens, spec_in = spec, 
           ppv_in = ppv, npv_in = npv)
    
  rec <- build_recipe(d = d_in, config = config_best)
  rec_prepped <- rec |> 
    prep(training = d_in, strings_as_factors = FALSE)
  
  feat_in <- rec_prepped |> 
    bake(new_data = NULL)
  
  model_best <- fit_best_model(config_best, feat = feat_in, "classification")
  
  feat_out <- rec_prepped |> 
    bake(new_data = d_out)   # no id_obs because not included in d_in
    
  preds_prob <- predict(model_best, feat_out,
                        type = "prob")

  # train calibration model train/test split on held in data
  set.seed(2468)
  cal_split <- d_in |> 
    group_initial_split(group = all_of(cv_group), prop = 3/4)
  d_cal_in <- training(cal_split) 
  d_cal_out <- testing(cal_split)
  
  feat_cal_in <- rec |> 
    prep(training = d_cal_in, strings_as_factors = FALSE) |> 
    bake(new_data = NULL) 
  
  feat_cal_out <- rec |> 
    prep(training = d_cal_in, strings_as_factors = FALSE) |> 
    bake(new_data = d_cal_out) 

  model_cal <- fit_best_model(config_best, feat = feat_cal_in, "classification")
  
  # iso calibration
  iso <- predict(model_cal, feat_cal_out,
                 type = "prob") |> 
    mutate(truth = feat_cal_out$y) |> 
    cal_estimate_isotonic(truth = truth,
                            estimate = dplyr::starts_with(".pred_"))
  preds_prob_iso <- preds_prob |> 
    cal_apply(iso)
  
  # logistic calibration
  logi <- predict(model_cal, feat_cal_out,
                  type = "prob") |>
    mutate(truth = feat_cal_out$y) |>
    cal_estimate_logistic(truth = truth,
                          estimate = dplyr::starts_with(".pred_"),
                             smooth = TRUE)
  preds_prob_logi <- preds_prob |>
    cal_apply(logi)

  # beta calibration
  beta <- predict(model_cal, feat_cal_out,
                  type = "prob") |>
    mutate(truth = feat_cal_out$y) |>
    cal_estimate_beta(truth = truth,
                      estimate = dplyr::starts_with(".pred_"),
                      smooth = TRUE)
  preds_prob_beta <- preds_prob |>
    cal_apply(beta)
  # cal_model <- betacal::beta_calibration(beta$.pred_yes, feat_cal_out$y, parameters = "abm")
  #   # Extract parameters
  #   a <- cal_model$map[1]
  #   b <- cal_model$map[2]
  #   m <- cal_model$map[3]
  # 
  #   # Apply calibration formula
  #   logit_calibrated <- a * log(preds_prob$.pred_yes) + b * log(1 - preds_prob$.pred_yes) + m
  #   beta_d_out <- 1 / (1 + exp(-logit_calibrated))
    
    # combine raw and calibrated probs
    probs_out <- tibble(id_obs = d_out$id_obs,
                        outer_split_num = rep(split_num, nrow(preds_prob)),
                        prob_raw = preds_prob[[str_c(".pred_", y_level_pos)]],
                        prob_iso = preds_prob_iso[[str_c(".pred_", y_level_pos)]],
                        prob_logi = preds_prob_logi[[str_c(".pred_", y_level_pos)]],
                        # prob_beta = beta_d_out,
                        prob_beta = preds_prob_beta[[str_c(".pred_", y_level_pos)]],
                        label = d_out$y) |> 
        mutate(label = fct_recode(label, "No lapse" = "no",
                                  "Lapse" = "yes"))

 # SHAP in held out fold
  # shaps_out <- SHAPforxgboost::shap.prep(xgb_model = extract_fit_engine(model_best),
  #                    X_train = feat_out |> select(-y) |>  as.matrix()) |> 
  #   # add id_obs by multiple of number of features
  #   mutate(id_obs = rep(d_out$id_obs, times = ncol(feat_out) - 1),
  #          split_num = split_num) |>  
  #   relocate(id_obs, split_num)
  
  return(list(probs_out = probs_out))
               
             # shaps_out = shaps_out))
}
```

```{r}
preds_1_old <- fit_predict_eval(1, splits, configs_best)
```


Compare
```{r}
hist(preds_1_new)
hist(preds_1_old)
```



**Next I will run all folds with this calibration on 2-week lag model and see how the plot looks.**
