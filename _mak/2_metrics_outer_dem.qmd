---
title: "Fits and evaluates best model configs for `r params$study` study across `r params$model` models in outer loop of nested for `r params$window` window and `r params$lead` lead and `r params$version` for comparing demographics"
author: "John Curtin & Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
params:
  study: "lag"
  window: "1week"
  lead: 0
  version: "v1"
  cv: "nested"
  model: "main" # "main" or "baseline"
editor_options: 
  chunk_output_type: console
---

### Code Status

In use for EMA study

### Notes
This script is altered to perform 6 repeates of 5-fold cross validation in outer loop.


### Set Up Environment

```{r set_params}
study <- params$study
data_type <- params$data_type
window <- params$window
lead <- params$lead 
version <- params$version
cv <- params$cv
model <- params$model
```

Function conflicts
```{r, packages_workflow}
#| message: false
#| warning: false

# handle conflicts
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
```

Packages for script
```{r, packages_script}
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(probably)
```

Source support functions
```{r source_functions}
# EDA
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")

# CHTC support functions
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/chtc/static_files/fun_chtc.R?raw=true")
```

Absolute paths
```{r, absolute_paths}
path_processed <- format_path(str_c("studydata/risk/data_processed/", study))
path_input <- format_path(str_c("studydata/risk/chtc/", study))
path_models <- format_path(str_c("studydata/risk/models/", study))
```


Chunk Defaults
```{r defaults}
#| include: false

knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

### Script Functions

Function to fit, predict, and calc metrics and preds
```{r function_2}
fit_predict_eval <- function(split_num, splits, configs_best){

  # write tmp file to repo to track progress through loop
  # delete this file when script is complete.  
  write_csv(tibble(stage = "eval",
                   outer_split_num = split_num, 
                   start_time = Sys.time()),
            here::here(path_models, str_c("tmp_metrics_outer_progress_", window)),
            append = TRUE)
  
  d_in <- training(splits$splits[[split_num]]) |> 
    select(-id_obs)  # not used for training; only needed in d_out to tag for later joins 
  d_out <- testing(splits$splits[[split_num]])
  
  config_best <- configs_best |> 
    slice(split_num) |> 
    rename(n_jobs_in = n_jobs, accuracy_in = accuracy, 
           bal_accuracy_in = bal_accuracy,
           roc_auc_in = roc_auc, sens_in =  sens, spec_in = spec, 
           ppv_in = ppv, npv_in = npv)
    
  rec <- build_recipe(d = d_in, config = config_best)
  rec_prepped <- rec |> 
    prep(training = d_in, strings_as_factors = FALSE)
  
  feat_in <- rec_prepped |> 
    bake(new_data = NULL)
  
  model_best <- fit_best_model(config_best, feat = feat_in, "classification")
  
  feat_out <- rec_prepped |> 
    bake(new_data = d_out)   # no id_obs because not included in d_in
  
  # metrics from raw (uncalibrated) predictions for held out fold
  preds_prob <- predict(model_best, feat_out,
                        type = "prob")
  preds_class <- predict(model_best, feat_out, type = "class")$.pred_class

  roc <- tibble(truth = feat_out$y, 
                prob = preds_prob[[str_c(".pred_", y_level_pos)]]) %>% 
      roc_auc(prob, truth = truth, event_level = "first") %>% 
      select(metric = .metric, 
             estimate = .estimate)
  
  cm <- tibble(truth = feat_out$y, estimate = preds_class) %>% 
    conf_mat(truth, estimate)
    
  metrics_out <- cm |> 
    summary(event_level = "first") |>   
    select(metric = .metric,
           estimate = .estimate) |> 
    filter(metric %in% c("sens", "spec", "ppv", "npv", "accuracy", "bal_accuracy")) |> 
    suppressWarnings() |>  # warning not about metrics we are returning
    bind_rows(roc) |> 
    pivot_wider(names_from = "metric", values_from = "estimate") |>    
    relocate(roc_auc, sens, spec, ppv, npv, accuracy, bal_accuracy) |> 
    bind_cols(config_best) |>
    relocate(outer_split_num, algorithm, feature_set, hp1, hp2, hp3, 
             resample) |> 
    relocate(accuracy_in, bal_accuracy_in, .after = last_col())

  if (model != "baseline") {
    # train calibration model train/test split on held in data
    # Skip for baseline models
    set.seed(2468)
    cal_split <- d_in |> 
      group_initial_split(group = all_of(cv_group), prop = 3/4)
    d_cal_in <- training(cal_split) 
    d_cal_out <- testing(cal_split)
  
    feat_cal_in <- rec %>% 
      prep(training = d_cal_in, strings_as_factors = FALSE) %>% 
      bake(new_data = NULL) 
  
    feat_cal_out <- rec %>% 
      prep(training = d_cal_in, strings_as_factors = FALSE) %>% 
      bake(new_data = d_cal_out) 

    model_cal <- fit_best_model(config_best, feat = feat_cal_in, "classification")
  
    # iso calibration
    iso <- predict(model_cal, feat_cal_out,
                   type = "prob") |> 
      mutate(truth = feat_cal_out$y) |> 
      cal_estimate_isotonic(truth = truth,
                            estimate = dplyr::starts_with(".pred_"))
    preds_prob_iso <- preds_prob |> 
      cal_apply(iso)
  
    # logistic calibration
    logi <- predict(model_cal, feat_cal_out,
                   type = "prob") |>
      mutate(truth = feat_cal_out$y) |>
      cal_estimate_logistic(truth = truth,
                             estimate = dplyr::starts_with(".pred_"),
                             smooth = TRUE)
    preds_prob_logi <- preds_prob |>
      cal_apply(logi)

    # beta calibration
    beta <- predict(model_cal, feat_cal_out,
                   type = "prob") |>
      mutate(truth = feat_cal_out$y) |>
      cal_estimate_beta(truth = truth,
                             estimate = dplyr::starts_with(".pred_"),
                             smooth = TRUE)
    preds_prob_beta <- preds_prob |>
      cal_apply(beta)

    # combine raw and calibrated probs
    probs_out <- tibble(id_obs = d_out$id_obs,
                        outer_split_num = rep(split_num, nrow(preds_prob)),
                        prob_raw = preds_prob[[str_c(".pred_", y_level_pos)]],
                        prob_iso = preds_prob_iso[[str_c(".pred_", y_level_pos)]],
                        prob_logi = preds_prob_logi[[str_c(".pred_", y_level_pos)]],
                        prob_beta = preds_prob_beta[[str_c(".pred_", y_level_pos)]],
                        label = d_out$y) |> 
      mutate(label = fct_recode(label, "No lapse" = "no",
                                "Lapse" = "yes"))
  

    return(list(probs_out = probs_out, 
                metrics_out = metrics_out)) 

  } else {
    probs_out <- tibble(id_obs = d_out$id_obs,
                        outer_split_num = rep(split_num, nrow(preds_prob)),
                        prob_raw = preds_prob[[str_c(".pred_", y_level_pos)]],
                        label = d_out$y) |> 
    mutate(label = fct_recode(label, "No lapse" = "no",
                                "Lapse" = "yes"))
    return(list(probs_out = probs_out, 
                metrics_out = metrics_out))
  }
}
```

### Read in aggregate CHTC metrics for inner folds
```{r read_inner_metrics}
metrics_raw <- 
  read_csv(here::here(path_models, str_c("inner_metrics_", window, 
                                   "_", lead, "_", version, "_", cv, "_", model, ".csv")), 
           col_types = "iiiiccdddcdddddddi") |> 
  glimpse()
```


### Identify best config for each outer fold (i.e., across inner folds)

Average metrics for each configuration across inner folds for each outer fold (XGBOOST only)
```{r best_model_1}
metrics_avg <- metrics_raw |> 
  filter(algorithm == "xgboost") |> 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample, outer_split_num) |> 
   summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                     median),
              n_jobs = n(), .groups = "drop") |> 
  relocate(outer_split_num, n_jobs) |> 
  arrange(outer_split_num, desc(roc_auc))
```

Best configuration for each outer fold
```{r best_model_2}
configs_best <- metrics_avg |> 
  group_by(outer_split_num) |> 
  arrange(desc(roc_auc)) |> 
  slice(1) |> 
  ungroup() |> 
  relocate(roc_auc, .before = accuracy)

configs_best |> print_kbl()
```

```{r}
configs_best |> pull(roc_auc) |> median()
```


### Fit best model for each outer fold and get/save metrics and preds

Get data from ANY batch (all same) and make splits

ASSUMPTIONS: 

* Data are same for all batches
* format_data() is same for all batches
* Assumes full recipe is for all algorithms is present in all training controls with branches/ifs to select proper algorithm specific steps

Map over all outer splits to get predicted probabilities, metrics, and SHAPs from held out outer folds.  Then save predicted probs, metrics, and SHAPs

NOTE: Delete `outer_metrics_*` or this code chunk won't run!
```{r eval_outer_folds}
if(!file.exists(here::here(path_models, str_c("outer_metrics_dem_6_x_5_", 
                                  window, "_", lead, "_", version, "_", 
                                  cv, "_", model, ".rds")))){ 
  
  # can source any training control given assumptions above
  batch_names <- list.dirs(path_input, full.names = FALSE, recursive = FALSE)
  batch_names <- batch_names[str_detect(batch_names, "train") & 
                               str_detect(batch_names, cv) &
                               str_detect(batch_names, version) &
                               str_detect(batch_names, window) &
                               str_detect(batch_names, 
                                          str_c(as.character(lead), "lag"))]
    if (model == "main") {
      batch_names <- batch_names[!str_detect(batch_names, "baseline")]
    }
 
  batch_name <- batch_names[1] # can source any batch given assumptions above
  path_batch <- here::here(path_input, batch_name)
  source(here::here(path_batch, "input", "training_controls.R"))
  # NOTE: training controls overwrites path_batch but it matches   
                    
  chunks <- str_split_fixed(data_trn, "\\.", n = Inf) # parse name from extensions
  if (length(chunks) == 2) {
    fn <- str_c("data_trn.", chunks[[2]])
  } else {
    fn <- str_c("data_trn.", chunks[[2]], ".", chunks[[3]])
  }
    
  # open based on file type
  if (str_detect(fn, "csv")) {
    d <- read_csv(here::here(path_batch, "input", fn), show_col_types = FALSE) 
  } else {
    d <- read_rds(here::here(path_batch, "input", fn))
  }
  
  d <- format_data(d) |> 
    arrange(label_num) |> 
    mutate(id_obs = 1:nrow(d))  # tmp add for linking obs
  
  # update outer resampling splits
  cv_outer_resample <- "6_x_5"
  splits <- d %>% 
    make_splits(cv_resample_type, cv_resample, cv_outer_resample, 
                cv_inner_resample, cv_group, seed_splits)
    
  all <- configs_best$outer_split_num |> 
    map(\(split_num) fit_predict_eval(split_num, splits, configs_best)) 
  
  
  rm(splits)  # save a bit of memory!
  
  write_csv(tibble(stage = "metrics_save",
                   outer_split_num = NA, 
                   start_time = Sys.time()),
            here::here(path_models, str_c("tmp_metrics_outer_progress_", window)),
            append = TRUE)
  metrics_out <- all |> 
    map(\(l) pluck(l, "metrics_out")) |> 
    list_rbind() |> 
    write_rds(here::here(path_models, str_c("outer_metrics_dem_6_x_5_", 
                                           window, "_", lead, "_", version, "_", 
                                           cv, "_", model, ".rds")))
  write_csv(tibble(stage = "probs_save",
                   outer_split_num = NA, 
                   start_time = Sys.time()),
            here::here(path_models, str_c("tmp_metrics_outer_progress_",window)),
            append = TRUE)  
  probs_out <- all |> 
    map(\(l) pluck(l, "probs_out")) |> 
    list_rbind() |> 
    write_rds(here::here(path_models, str_c("outer_preds_dem_6_x_5_", 
                                           window, "_", lead, "_", version, "_", 
                                           cv, "_", model, ".rds")))

    write_csv(tibble(stage = "shaps_save",
                   outer_split_num = NA, 
                   start_time = Sys.time()),
            here::here(path_models, str_c("tmp_metrics_outer_progress_", window)),
            append = TRUE)    
    
} else {
  message("Resampled performance from nested CV previously calculated")
  
  metrics_out <- read_rds(here::here(path_models, str_c("outer_metrics_", 
                                                       window, "_", lead, "_", 
                                                       version, "_", 
                                                       cv, "_", model, ".rds")))
}
```


### Final Review performance eval from outer loop

Done more in depth later script but here is a quick look
```{r print_metrics}
metrics_out |> 
  print_kbl()

metrics_out |> 
  summarize(median(roc_auc), mean(roc_auc), min(roc_auc), max(roc_auc))

metrics_out |> 
  ggplot(aes(x = roc_auc)) +
  geom_histogram(bins = 10)
```

```{r}
# delete tracking file
if(file.exists(here::here(path_models, str_c("tmp_metrics_outer_progress_", window)))) {
  file.remove(here::here(path_models, str_c("tmp_metrics_outer_progress_", window)))
}
```

