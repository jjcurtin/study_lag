---
title: "Reprex of callibration error"
author: "Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
params:
  study: "lag"
  window: "1day"
  lead: 336
  version: "v3"
  cv: "nested"
  model: "main" # "main" or "baseline"
editor_options: 
  chunk_output_type: console
---



### Notes
In the 336 lagged models getting following error for split number 27:

`Error in uniroot(function(mh) b * log(1 - mh) - a * log(mh) - inter, c(1e-16,  :  f() values at end points not of opposite sign`    

Error occurs when running function `cal_estimate_beta()`   


Notes:   
- appears to be a combination of split and model conifiguration
  - model: config hp1 .01, hp2 2, hp3 30, resample down_1
  - alt config did not work: .01, 2, 30, down_2
  - alt config worked: .01, 3, 20, none
- Adequate proportion of lapses in both held in and held out sets (6-7%)
- no probability values of 0 or 1


### Set Up Environment

```{r set_params}
study <- params$study
data_type <- params$data_type
window <- params$window
lead <- params$lead 
version <- params$version
cv <- params$cv
model <- params$model
```

Function conflicts
```{r, packages_workflow}
#| message: false
#| warning: false

# handle conflicts
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
```

Packages for script
```{r, packages_script}
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(probably)
```

Source support functions
```{r source_functions}
# EDA
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")

# CHTC support functions
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/chtc/static_files/fun_chtc.R?raw=true")
```

Absolute paths
```{r, absolute_paths}
path_processed <- format_path(str_c("studydata/risk/data_processed/", study))
path_input <- format_path(str_c("studydata/risk/chtc/", study))
path_models <- format_path(str_c("studydata/risk/models/", study))
```



### Read in aggregate CHTC metrics for inner folds
```{r read_inner_metrics}
metrics_raw <- 
  read_csv(here::here(path_models, str_c("inner_metrics_", window, 
                                   "_", lead, "_", version, "_", cv, "_", model, ".csv")), 
           col_types = "iiiiccdddcdddddddi") |> 
  glimpse()
```


### Identify best config for each outer fold (i.e., across inner folds)

```{r best_model_1}
metrics_avg <- metrics_raw |> 
  filter(algorithm == "xgboost") |> 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample, outer_split_num) |> 
   summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                     median),
              n_jobs = n(), .groups = "drop") |> 
  relocate(outer_split_num, n_jobs) |> 
  arrange(outer_split_num, desc(roc_auc))
```

Best configuration for each outer fold
```{r best_model_2}
configs_best <- metrics_avg |> 
  group_by(outer_split_num) |> 
  arrange(desc(roc_auc)) |> 
  slice(1) |> 
  ungroup() |> 
  relocate(roc_auc, .before = accuracy)

configs_best |> print_kbl()
```

### Load data and make splits
```{r}
batch_names <- list.dirs(path_input, full.names = FALSE, recursive = FALSE)
batch_name <- batch_names[str_detect(batch_names, "train") & 
                               str_detect(batch_names, cv) &
                               str_detect(batch_names, version) &
                               str_detect(batch_names, window) &
                               str_detect(batch_names, 
                                         str_c(as.character(lead), "lag"))]

source(here::here(path_input, batch_name, "input", "training_controls.R"))
                    
d <- read_csv(here::here(path_batch, "input", "data_trn.csv"), 
              show_col_types = FALSE) 

d <- format_data(d) |> 
  arrange(label_num) |> 
  mutate(id_obs = 1:nrow(d))  
  
splits <- d |> 
    make_splits(cv_resample_type, cv_resample, cv_outer_resample, 
                cv_inner_resample, cv_group, seed_splits)
```


### Reproduce error 

Fit model and get beta probabilities for split 27
```{r function_2}

split_num <- 25

d_in <- training(splits$splits[[split_num]]) |> 
    select(-id_obs) 
  
d_out <- testing(splits$splits[[split_num]])
  
config_best <- configs_best |> 
  slice(split_num)
    
rec <- build_recipe(d = d_in, config = config_best)
  
rec_prepped <- rec |> 
  prep(training = d_in, strings_as_factors = FALSE)
  
feat_in <- rec_prepped |> 
  bake(new_data = NULL)
  
model_best <- fit_best_model(config_best, feat = feat_in, "classification")
  
feat_out <- rec_prepped |> 
  bake(new_data = d_out)   # no id_obs because not included in d_in
    
preds_prob <- predict(model_best, feat_out,
                      type = "prob")


# train calibration model train/test split on held in data

set.seed(2468)
cal_split <- d_in |> 
  group_initial_split(group = all_of(cv_group), prop = 3/4)

d_cal_in <- training(cal_split) 
  
d_cal_out <- testing(cal_split)
  
feat_cal_in <- rec |> 
    prep(training = d_cal_in, strings_as_factors = FALSE) %>% 
    bake(new_data = NULL) 
  
feat_cal_out <- rec |>
    prep(training = d_cal_in, strings_as_factors = FALSE) %>% 
    bake(new_data = d_cal_out) 

    
model_cal <- fit_best_model(config_best, feat = feat_cal_in, "classification")
  

# beta calibration
beta <- predict(model_cal, feat_cal_out,
                type = "prob") |>
   mutate(truth = feat_cal_out$y) 


# new way
beta_model <- betacal::beta_calibration(p = beta$.pred_yes,
                          y = beta$truth) 

preds_prob_beta <- betacal::beta_predict(p = preds_prob$.pred_yes,
                                    calib = beta_model) |> 
  enframe(name = NULL,value = ".pred_yes")


# old way
beta_model_old <- beta |>
      cal_estimate_beta(truth = truth,
                             estimate = dplyr::starts_with(".pred_yes"),
                             smooth = TRUE)

preds_prob_beta_old <- preds_prob |>
  select(.pred_yes) |> 
  cal_apply(beta_model_old)
```


Does not match!
