---
title: "Fits and evaluates final model selected from inner resampling method on all data"
author: "Kendra Wyant and John Curtin"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
params:
  study: "lag"
  window: "1day"
  lead: 24
  version: "v3"
  cv: "kfold"
  model: "strat_lh_final"
editor_options: 
  chunk_output_type: console
---

### Code Status

In use for Lag study

### Notes
This script fits and evaluates final model selected from inner resampling method and fit on all data. It uses 1 x 5-fold CV to return a single probability for each participant. Output saves raw and calibrated probabilities.

This script creates the following files in the `models` folder

* outer_preds_*.rds
* outer_shaps_*.rds
* outer_shapsgrp_*.rds



### Set Up Environment

```{r}
study <- params$study
data_type <- params$data_type
window <- params$window
lead <- params$lead 
version <- params$version
cv <- params$cv
model <- params$model
```


Packages for script
```{r}
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
options(conflicts.policy = "depends.ok")
library(probably)
library(betacal)
```

Source support functions
```{r}
# EDA
source("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")

# CHTC support functions
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/chtc/static_files/fun_chtc.R?raw=true")
```

Absolute paths
```{r}
path_processed <- format_path(str_c("risk/data_processed/", study))
path_input <- format_path(str_c("risk/chtc/", study))
path_models <- format_path(str_c("risk/models/", study))
```


Chunk Defaults
```{r}
#| include: false

knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

### Script Functions

Function to fit and predict
```{r}
fit_predict_eval <- function(split_num, splits, config_best, rec){


  d_in <- training(splits$splits[[split_num]])
  
  d_out <- testing(splits$splits[[split_num]])

  rec_prepped <- rec |> 
    prep(training = d_in)

  feat_in <- rec_prepped |> 
    bake(new_data = NULL)

  model_best <- fit_best_model(config_best, feat = feat_in, "classification")

  feat_out <- rec_prepped |> 
    bake(new_data = d_out)   # no id_obs because not included in d_in
  
  preds_prob <- predict(model_best, feat_out,
                        type = "prob")

  # train calibration model 
  set.seed(2468)
  cal_split <- d_in |>
    group_initial_split(group = all_of(cv_group), strata = strat, 
                        prop = 3/4)
  d_cal_in <- training(cal_split)
  d_cal_out <- testing(cal_split)

  rec_prep <- rec  |>
      prep(training = d_cal_in)

  feat_cal_in <- rec_prep |>
      bake(new_data = NULL)

  feat_cal_out <- rec_prep %>%
      bake(new_data = d_cal_out)

  model_cal <- fit_best_model(config_best, feat = feat_cal_in, "classification")
  
  # iso calibration
  iso <- predict(model_cal, feat_cal_out,
                 type = "prob") |> 
    mutate(truth = feat_cal_out$y) |> 
    cal_estimate_isotonic(truth = truth,
                            estimate = dplyr::starts_with(".pred_"))
  preds_prob_iso <- preds_prob |> 
    cal_apply(iso)
  
  # logistic calibration
  logi <- predict(model_cal, feat_cal_out,
                 type = "prob") |> 
    mutate(truth = feat_cal_out$y) |> 
    cal_estimate_logistic(truth = truth,
                          estimate = dplyr::starts_with(".pred_"),
                             smooth = TRUE)
  preds_prob_logi <- preds_prob |>
    cal_apply(logi)

  # beta calibration
  beta <- predict(model_cal, feat_cal_out,
                 type = "prob") |> 
    mutate(truth = feat_cal_out$y) |> 
    cal_estimate_beta(truth = truth,
                      estimate = dplyr::starts_with(".pred_"),
                      smooth = TRUE)
   preds_prob_beta <- preds_prob |>
     cal_apply(beta)
    
   # combine raw and calibrated probs
   tibble(id_obs = d_out$label_num,
                   prob_raw = preds_prob[[str_c(".pred_", y_level_pos)]],
                   prob_iso = preds_prob_iso[[str_c(".pred_", y_level_pos)]],
                   prob_logi = preds_prob_logi[[str_c(".pred_", y_level_pos)]],
                   prob_beta = preds_prob_beta[[str_c(".pred_", y_level_pos)]],
                   label = d_out$y) |> 
      mutate(label = fct_recode(label, "No lapse" = "no",
                                "Lapse" = "yes"))

}
```

### Read in aggregate CHTC metrics for inner folds
```{r}
# rename inner_metrics
metrics_raw <- 
  read_csv(here::here(path_models, str_c("inner_metrics_", window, 
                                   "_", lead, "_", version, "_", cv, "_",
                                   model, ".csv")), 
           col_types = "iiiiccdddcdddddddi") |> 
  glimpse()
```


### Identify best config for each outer fold (i.e., across inner folds)

Average metrics for each configuration across inner folds for each outer fold (XGBOOST only)
```{r}
metrics_avg <- metrics_raw |> 
  filter(algorithm == "xgboost") |> 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample) |> 
   summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                     median),
              n_jobs = n(), .groups = "drop") |> 
  relocate(n_jobs) |> 
  arrange(desc(roc_auc))
```

Best configuration from kfold
```{r best_model_2}
config_best <- metrics_avg |> 
  slice(1) 

config_best |> glimpse()
```


### Fit best model for each outer fold and get/save preds

ASSUMPTIONS: 

* Data are same for all batches
* format_data() is same for all batches
* Assumes full recipe is for all algorithms is present in all training controls with branches/ifs to select proper algorithm specific steps

Map over all outer splits to get predicted probabilities and SHAPs from held out outer folds.  Then save predicted probs and SHAPs

NOTE: Delete `outer_metrics_*` or this code chunk won't run!
```{r}
# can source any training control given assumptions above
batch_names <- list.dirs(path_input, full.names = FALSE, recursive = FALSE)
batch_names <- batch_names[str_detect(batch_names, "train") & 
                           str_detect(batch_names, cv) &
                           str_detect(batch_names, version) &
                           str_detect(batch_names, window) &
                           str_detect(batch_names, model) &
                           str_detect(batch_names, 
                                      str_c(as.character(lead), "lag"))]
  
batch_name <- batch_names[1] # can source any batch given assumptions above
path_batch <- here::here(path_input, batch_name)
source(here::here(path_batch, "input", "training_controls.R"))
source("https://github.com/jjcurtin/lab_support/blob/main/chtc/static_files/fun_chtc.R?raw=true")
# NOTE: training controls overwrites path_batch but it matches   
  
chunks <- str_split_fixed(data_trn, "\\.", n = Inf) # parse name from extensions
if (length(chunks) == 2) {
  fn <- str_c("data_trn.", chunks[[2]])
} else {
  fn <- str_c("data_trn.", chunks[[2]], ".", chunks[[3]])
}


# open based on file type
if (str_detect(fn, "csv")) {
  d <- read_csv(here::here(path_batch, "input", fn), show_col_types = FALSE) 
} else {
  d <- read_rds(here::here(path_batch, "input", fn))
}
  
  

d <- format_data(d) |> 
  arrange(label_num) 

# can use any data to build recipe  
rec <- build_recipe(d = d, config = config_best)

splits <- d |> 
    make_splits(cv_resample_type = "kfold", cv_resample = "1_x_5", 
                cv_outer_resample = NULL, cv_inner_resample = NULL, 
                cv_group, cv_strat = cv_strat, the_seed = seed_splits)

    
all <- 1:length(splits$splits) |> 
  map(\(split_num) fit_predict_eval(split_num, splits, config_best, rec)) |> 
  list_rbind()

all |> 
  write_rds(here::here(path_models, str_c("final_preds_", cv, "_1_x_5",
                                           "_", window, "_", lead, "_", 
                                           version, "_", model, ".rds")))
```

Check probabilities
```{r}
hist(all$prob_raw)
```


Check calibration
```{r}
bin_width = 0.10

all |> 
  mutate(bins = cut(prob_beta, breaks = seq(0, 1, bin_width)), 
         lapse = if_else(label == "Lapse", 1, 0)) |> 
  group_by(bins)  |> 
  summarize(mean_lapse = mean(lapse),
            .groups = "drop") |>
  mutate(bins = as.numeric(bins),
         midpoints = bin_width/2 + bin_width * (bins - 1))  |> 
  ggplot(data = _, aes(x = midpoints, y = mean_lapse)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
  geom_line() +
  geom_point() +
  labs(x = "Predicted Lapse Probability (bin mid-point)",
       y = "Observed Lapse Probability") +
  scale_x_continuous(breaks = seq(0, 1, bin_width),
                     limits = c(0, 1)) +
  scale_y_continuous(breaks = seq(0, 1, bin_width),
                     limits = c(0, 1)) 
```


