---
title: "Fits and evaluates best model configs for `r params$study` study across `r params$model` models in outer loop of nested for `r params$window` window and `r params$lead` lead and `r params$version`"
author: "John Curtin & Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
params:
  study: "lag"
  window: "1day"
  lead: 336
  version: "v3"
  cv: "nested"
  model: "strat_lh"

editor_options: 
  chunk_output_type: console
---

### Code Status

In use for Lag study

### Notes
This script reads in CHTC performance metrics from the inner loops of CV, selects the best model configuration for each outer loop, trains those models and predicts into the outer held-out folds.  It returns raw and calibrated predicted probabilities and Shapley values. AuROCs based on calibrated probabilities are also returned.

This script creates the following files in the `models` folder

* outer_preds_*.rds
* test_auroc_*.csv



### Set Up Environment

```{r}
study <- params$study
data_type <- params$data_type
window <- params$window
lead <- params$lead 
version <- params$version
cv <- params$cv
model <- params$model
```

Function conflicts
```{r}
#| message: false
#| warning: false

# handle conflicts
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
```

Packages for script
```{r}
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(probably)
library(betacal)
```

Source support functions
```{r}
# EDA
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")

# CHTC support functions
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/chtc/static_files/fun_chtc.R?raw=true")
```

Absolute paths
```{r}
path_processed <- format_path(str_c("studydata/risk/data_processed/", study))
path_input <- format_path(str_c("studydata/risk/chtc/", study))
path_models <- format_path(str_c("studydata/risk/models/", study))
```


Chunk Defaults
```{r}
#| include: false

knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

### Script Functions

Function to fit and predict
```{r}
fit_predict_eval <- function(split_num, splits, configs_best, 
                             calibration = TRUE){

  # write tmp file to repo to track progress through loop
  # delete this file when script is complete.  
  write_csv(tibble(stage = "eval",
                   outer_split_num = split_num, 
                   start_time = Sys.time()),
            here::here(path_models, str_c("tmp_metrics_outer_progress_", window)),
            append = TRUE)
  
  d_in <- training(splits$splits[[split_num]]) |> 
    select(-id_obs)  # not used for training; only needed in d_out to tag for later joins 
  d_out <- testing(splits$splits[[split_num]])
  
  config_best <- configs_best |> 
    slice(split_num) |> 
    rename(n_jobs_in = n_jobs, accuracy_in = accuracy, 
           bal_accuracy_in = bal_accuracy,
           roc_auc_in = roc_auc, sens_in =  sens, spec_in = spec, 
           ppv_in = ppv, npv_in = npv)
    
  rec <- build_recipe(d = d_in, config = config_best)
  rec_prepped <- rec |> 
    prep(training = d_in, strings_as_factors = FALSE)
  
  feat_in <- rec_prepped |> 
    bake(new_data = NULL)
  
  model_best <- fit_best_model(config_best, feat = feat_in, "classification")
  
  feat_out <- rec_prepped |> 
    bake(new_data = d_out)   # no id_obs because not included in d_in
    
  preds_prob <- predict(model_best, feat_out,
                        type = "prob")

  
  return(list(probs_out = probs_out))
}
```

### Read in aggregate CHTC metrics for inner folds
```{r read_inner_metrics}
metrics_raw <- 
  read_csv(here::here(path_models, str_c("inner_metrics_", window, 
                                   "_", lead, "_", version, "_", cv, "_", model, ".csv")), 
           col_types = "iiiiccdddcdddddddi") |> 
  glimpse()
```


### Identify best config for each outer fold (i.e., across inner folds)

Average metrics for each configuration across inner folds for each outer fold (XGBOOST only)
```{r best_model_1}
metrics_avg <- metrics_raw |> 
  filter(algorithm == "xgboost") |> 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample, outer_split_num) |> 
   summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                     median),
              n_jobs = n(), .groups = "drop") |> 
  relocate(outer_split_num, n_jobs) |> 
  arrange(outer_split_num, desc(roc_auc))
```

Best configuration for each outer fold
```{r best_model_2}
configs_best <- metrics_avg |> 
  group_by(outer_split_num) |> 
  arrange(desc(roc_auc)) |> 
  slice(1) |> 
  ungroup() |> 
  relocate(roc_auc, .before = accuracy)

configs_best |> print_kbl()
```

```{r}
configs_best |> pull(roc_auc) |> mean()
configs_best |> pull(roc_auc) |> median()
```

```{r}
configs_best |> 
  ggplot(aes(x = roc_auc)) +
  geom_histogram(bins = 10)
```

### Fit best model for each outer fold and get/save preds

Get data from ANY batch (all same) and make splits

ASSUMPTIONS: 

* Data are same for all batches
* format_data() is same for all batches
* Assumes full recipe is for all algorithms is present in all training controls with branches/ifs to select proper algorithm specific steps

Map over all outer splits to get predicted probabilities and SHAPs from held out outer folds.  Then save predicted probs and SHAPs

NOTE: Delete `outer_metrics_*` or this code chunk won't run!
```{r}
# can source any training control given assumptions above
batch_names <- list.dirs(path_input, full.names = FALSE, recursive = FALSE)
batch_names <- batch_names[str_detect(batch_names, "train") & 
                           str_detect(batch_names, cv) &
                           str_detect(batch_names, version) &
                           str_detect(batch_names, window) &
                           str_detect(batch_names, model) &
                           str_detect(batch_names, 
                                      str_c(as.character(lead), "lag"))]
  
batch_name <- batch_names[1] # can source any batch given assumptions above
path_batch <- here::here(path_input, batch_name)
source(here::here(path_batch, "input", "training_controls.R"))
source(here::here(path_batch, "input", "fun_chtc.R"))
# NOTE: training controls overwrites path_batch but it matches   
  
if(!file.exists(here::here(path_models, str_c("outer_preds_", 
                                              cv_outer_resample,
                                              "_", window, "_", lead, "_", 
                                              version, "_", cv, "_", model,
                                              ".rds")))){ 
  
  chunks <- str_split_fixed(data_trn, "\\.", n = Inf) # parse name from extensions
  if (length(chunks) == 2) {
    fn <- str_c("data_trn.", chunks[[2]])
  } else {
    fn <- str_c("data_trn.", chunks[[2]], ".", chunks[[3]])
  }
    
  # open based on file type
  if (str_detect(fn, "csv")) {
    d <- read_csv(here::here(path_batch, "input", fn), show_col_types = FALSE) 
  } else {
    d <- read_rds(here::here(path_batch, "input", fn))
  }
  
  

  # Read in lapse_strat.csv if stratifying
  if(!is.null(cv_strat)) {
    lapse_strat <- read_csv(here::here(path_batch, "input/lapse_strat.csv"),
                            show_col_types = FALSE)
  } else {
    lapse_strat <- NULL
  }

 d <- format_data(d, lapse_strat) |> 
    arrange(label_num) |> 
    mutate(id_obs = 1:nrow(d))  # tmp add for linking obs
  
  
 splits <- d |> 
    make_splits(cv_resample_type, cv_resample, cv_outer_resample, 
              cv_inner_resample, cv_group, cv_strat = cv_strat,
              the_seed = seed_splits)

    
  all <- configs_best$outer_split_num |> 
    map(\(split_num) fit_predict_eval(split_num, splits, configs_best)) 
  
  
  probs_out <- all |> 
    map(\(l) pluck(l, "probs_out")) |> 
    list_rbind() |> 
    write_rds(here::here(path_models, str_c("outer_preds_", cv_outer_resample,
                                           "_", window, "_", lead, "_", 
                                           version, "_", cv, "_", model,
                                           ".rds")))
 
} else {
  message("Resampled performance from nested CV previously calculated")
  
  probs_out <- read_rds(here::here(path_models, str_c("outer_preds_",
                                                    cv_outer_resample, "_",
                                                     window, "_", lead, "_",
                                                     version, "_",
                                                     cv, "_", model, ".rds")))
}
```


Get test auROCs for main models    

This calculates auROC using test sets (out loop held out folds). Uses raw probabilities.

```{r}
metrics_test <- probs_out |> 
  nest(.by = outer_split_num, .key = "preds") |> 
  mutate(auroc = map(preds, \(preds) roc_auc(preds, prob_raw, 
                                             truth = label))) |> 
  select(-preds) |> 
  unnest(auroc) |> 
  select(outer_split_num, roc_auc = .estimate)

metrics_test |> 
  arrange(outer_split_num) |> 
  write_csv(here::here(path_models, str_c("test_auroc_", cv_outer_resample,
                                          "_", window, "_", lead, "_", 
                                          version, "_", cv, "_", model, 
                                          ".csv"))) 

median(metrics_test$roc_auc) 
```




Delete tracking file
```{r}
if(file.exists(here::here(path_models, str_c("tmp_metrics_outer_progress_", window)))) {
  file.remove(here::here(path_models, str_c("tmp_metrics_outer_progress_", window)))
}
```

