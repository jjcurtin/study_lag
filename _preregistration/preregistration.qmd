---
title: "Preregistration: Machine learning models for lagged predictions of next week alcohol use" 
author: "Kendra Wyant"
date: "`r April 11, 2024"
number-sections: true
format: 
 pdf: 
   toc: true
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| echo: false

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(source("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true"))
suppressPackageStartupMessages(library(tidyposterior))

path_models_lag <- format_path(str_c("studydata/risk/models/lag"))
path_models_ema <- format_path(str_c("studydata/risk/models/ema"))
```


```{r}
#| label: ema-results
#| echo: false

val_auc_week <- read_rds(file.path(path_models_ema, 
                                  "outer_metrics_1week_0_v5_nested_main.rds")) |> 
  select(outer_split_num, contains("auc_in"))

test_metrics_week <- read_csv(file.path(path_models_ema, 
                                        "test_metrics_1week_0_v5_nested.csv"), 
                              col_types = cols())

test_metrics_day <- read_csv(file.path(path_models_ema, 
                                       "test_metrics_1day_0_v5_nested.csv"),
                             col_types = cols())

test_metrics_hour <- read_csv(file.path(path_models_ema, 
                                        "test_metrics_1hour_0_v5_nested.csv"),
                              col_types = cols())

metrics <- test_metrics_week |> 
  mutate(model = "Week") |> 
  bind_rows(test_metrics_day |> 
              mutate(model = "Day")) |> 
  bind_rows(test_metrics_hour |> 
              mutate(model = "Hour")) |> 
  group_by(.metric, model) |> 
  summarize(median = median(.estimate), .groups = "drop") |> 
  pivot_wider(names_from = model, values_from = median) |> 
  select(.metric, Week, Day, Hour)

metrics <- metrics[c(4,5,6, 1, 3, 2),]
```

```{r}
#| label: inner-aucs
#| echo: false

inner_lag <- read_csv(file.path(path_models_lag, 
                                 "inner_metrics_1week_0_v1_nested_main.csv"),
                       col_types = cols()) |> 
  mutate(lag = 0) |> 
  bind_rows(read_csv(file.path(path_models_lag, 
                                 "inner_metrics_1week_24_v1_nested_main.csv"),
                       col_types = cols()) |> 
  mutate(lag = 24)) |> 
  bind_rows(read_csv(file.path(path_models_lag, 
                                 "inner_metrics_1week_72_v1_nested_main.csv"),
                       col_types = cols()) |> 
  mutate(lag = 72)) |> 
  bind_rows(read_csv(file.path(path_models_lag, 
                                   "inner_metrics_1week_168_v1_nested_main.csv"),
                         col_types = cols()) |> 
  mutate(lag = 168)) |> 
  bind_rows(read_csv(file.path(path_models_lag, 
                                   "inner_metrics_1week_336_v1_nested_main.csv"),
                         col_types = cols()) |> 
  mutate(lag = 336)) |> 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample, lag) %>% 
    summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                     median),
              n_jobs = n(), .groups = "drop") |> 
  group_by(lag) |> 
  arrange(desc(roc_auc)) |> 
  slice(1) |> 
  select(lag, algorithm, hp1, hp2, hp3, resample, roc_auc)
```

```{r}
#| echo: false

# We simulated truncated normal distributions (upper limit set to 1) with means decreasing as lag increases. Standard deviations were set to .04 for all models (based on SD of outer fold auROCs for our no lag one week model in our previous paper).
set.seed(102030)
sim_auroc <- tibble(lag = rep(0, 10),
                    repeat_num = rep(1, 10),
                    fold_num = seq(1:10),
                    auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .88, sd = .04)) |> 
  bind_rows(tibble(lag = rep(0, 10),
                   repeat_num = rep(2, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .88, sd = .04))) |> 
   bind_rows(tibble(lag = rep(0, 10),
                   repeat_num = rep(3, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .88, sd = .04))) |> 
   bind_rows(tibble(lag = rep(24, 10),
                   repeat_num = rep(1, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .87, sd = .04))) |> 
   bind_rows(tibble(lag = rep(24, 10),
                   repeat_num = rep(2, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0,mean = .87, sd = .04))) |> 
   bind_rows(tibble(lag = rep(24, 10),
                   repeat_num = rep(3, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .87, sd = .04))) |> 
  bind_rows(tibble(lag = rep(72, 10),
                   repeat_num = rep(1, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .85, sd = .04))) |> 
   bind_rows(tibble(lag = rep(72, 10),
                   repeat_num = rep(2, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .85, sd = .04))) |> 
   bind_rows(tibble(lag = rep(72, 10),
                   repeat_num = rep(3, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .85, sd = .04))) |> 
  bind_rows(tibble(lag = rep(168, 10),
                   repeat_num = rep(1, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .82, sd = .04))) |> 
   bind_rows(tibble(lag = rep(168, 10),
                   repeat_num = rep(2, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .82, sd = .04))) |> 
   bind_rows(tibble(lag = rep(168, 10),
                   repeat_num = rep(3, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0,  mean = .82, sd = .04))) |> 
   bind_rows(tibble(lag = rep(336, 10),
                   repeat_num = rep(1, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .78, sd = .04))) |> 
   bind_rows(tibble(lag = rep(336, 10),
                   repeat_num = rep(2, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .78, sd = .04))) |> 
   bind_rows(tibble(lag = rep(336, 10),
                   repeat_num = rep(3, 10),
                   fold_num = seq(1:10),
                   auroc = truncnorm::rtruncnorm(n = 10, b = 1.0, mean = .78, sd = .04)))
```

Jump to @sec-prereg for preregistered analyses.  


## Background

In a previous manuscript ([Wyant et al., in press](https://osf.io/preprints/psyarxiv/cgsf7)), we developed three separate models that provide hour-by-hour probabilities of future goal-inconsistent alcohol use (i.e., lapses) with increasing temporal precision: lapses in the next week, next day, and next hour. Model features were engineered from raw scores and longitudinal change in responses to 4X daily ecological momentary assessment (EMA). These features were derived to measure theoretically-implicated risk factors including past use, craving, past pleasant events, past and future risky situations, past and future stressful events, emotional valence and arousal, and self-efficacy. We demonstrated that it was possible to predict future alcohol lapses in the next week, day, and hour with high sensitivity and specificity (area under the receiver operating curves [auROCs] of `r sprintf("%1.2f", test_metrics_week |> filter(.metric == "roc_auc") |> pull(.estimate) |> median())`, `r sprintf("%1.2f", test_metrics_day |> filter(.metric == "roc_auc") |> pull(.estimate) |> median())`, and `r sprintf("%1.2f", test_metrics_hour |> filter(.metric == "roc_auc") |> pull(.estimate) |> median())` respectively).     


This study is an extension of our previous work. Our previous models predicted the probability of lapse in the next week, next day, and next hour starting now. While, next hour and next day models are well-positioned to identify and recommend just-in-time interventions to address these immediate risks, the next week model may not have sufficient temporal specificity to recommend immediate patient action. We believe its clinical utility may improve if we shift this coarser window width into the future. This "time-lagged" model could provide patients with increased lead time to implement multi-modal supports that might not be immediately available to them (e.g., schedule therapy appointment, request support from an AA sponsor). The current study proposes training models to predict the probability of lapse at any point during a week window that is shifted 24 hours (one day), 72 hours (three days), 168 hours (one week), or 336 hours (two weeks) into the future. Our primary goal is to assess how model performance changes as lag increases.     


## Modeling Decisions
Our primary performance metric for model selection and evaluation will be auROC.   

We will consider four candidate statistical algorithms including elastic net, XGBoost, regularized discriminant analysis (rda), and single layer neural networks. Candidate model configurations will differ across sensible values for key hyperparameters. They will also differ on outcome resampling method (i.e., no resampling and up-sampling and down-sampling of the outcome using majority/no lapse to minority/lapse ratios ranging from 1:1 to 2:1). 

We will use participant-grouped, nested cross-validation for model training, selection, and evaluation with auROC. We will use 1 repeat of 10-fold cross-validation for the inner loops (i.e., *validation* sets for model selection) and 3 repeats of 10-fold cross-validation for the outer loop (i.e., *test* sets for model evaluation).     

Best model configurations (i.e., the best performing combination of algorithm, hyperparameter values, and resampling method) will be selected using median auROC across the 10 validation sets.  Final performance evaluation of those best model configurations used median auROC across the 30 test sets. 


## Progress at Time of Preregistration

We have fit the inner nested k-fold cross-validation loop for all XGBoost, glmnet, and RDA model configurations.   

Next, we will train our remaining model configurations using the neural network algorithm and evaluate our 30 best model configurations for each model in the outer cross-validation loops. Importantly, we have not fit any models in the 30 outer folds or conducted any planned data analyses at the time of this preregistration. 


## Preregistered Analyses {#sec-prereg}

The following sections contain our preregistered analyses. We both describe this process in the text and show sample code using simulated data to be as precise as possible.

### Model Evaluation

We will use a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) for auROC for the five best models (no lag and 24 hour, 72 hour, 168 hour, and 336 hour lagged predictions) on the 30 held-out test sets. We will plot the posterior probability distributions. We will report the median posterior probability and 95% credible intervals (CI) for auROC for our baseline (no lag) model and each lagged model. The median posterior probability for auROC will represent our best estimate for the magnitude of the auROC parameter for each model. If the confidence intervals do not contain .5 (chance performance), this will suggest our model is capturing signal in the data.  


#### Sample code with simulated data

Below we demonstrate our model evaluation process for auROC with 30 simulated held-out tests sets for each model. We also show how we will write up our results using the simulated data. 


```{r}
#| output: false

# Repeated CV (id = repeat, id2 = fold within repeat)
# with a common variance:  statistic ~ model + (model | id2/id)
set.seed(101)
pp <- sim_auroc |> 
  pivot_wider(names_from = lag, values_from = auroc) |> 
  rename(id = repeat_num,
         id2 = fold_num) |> 
  perf_mod(formula = statistic ~ model + (1 | id2/id),
         transform = tidyposterior::logit_trans,  # for skewed & bounded AUC
         iter = 2000, chains = 4, adapt_delta = .99, # defaults but may increase to fix divergence issues
         family = gaussian, 
)  
```

```{r}
sim_auroc_perf <- sim_auroc |>
  group_by(lag) |>
  summarize(median = median(auroc), min = min(auroc),
            max = max(auroc), IQR = IQR(auroc)) 

sim_auroc_perf
```


The median auROC across the 30 test sets for our baseline model was high (median=`r sprintf("%1.3f", sim_auroc_perf |> filter(lag == 0) |> pull(median))`, IQR=`r sprintf("%1.3f", sim_auroc_perf |> filter(lag == 0) |> pull(IQR))`, consistent with our previous study. Performance across our lagged models was moderate to high for the 24 hour lag (median=`r sprintf("%1.3f", sim_auroc_perf |> filter(lag == 24) |> pull(median))`, IQR=`r sprintf("%1.3f", sim_auroc_perf |> filter(lag == 24) |> pull(IQR))`, 72 hour lag (median=`r sprintf("%1.3f", sim_auroc_perf |> filter(lag == 72) |> pull(median))`, IQR=`r sprintf("%1.3f", sim_auroc_perf |> filter(lag == 72) |> pull(IQR))`, 168 hour lag (median=`r sprintf("%1.3f", sim_auroc_perf |> filter(lag == 168) |> pull(median))`, IQR=`r sprintf("%1.3f", sim_auroc_perf |> filter(lag == 168) |> pull(IQR))`, and 336 hour lag (median=`r sprintf("%1.3f", sim_auroc_perf |> filter(lag == 336) |> pull(median))`, IQR=`r sprintf("%1.3f", sim_auroc_perf |> filter(lag == 336) |> pull(IQR))`. 

```{r}
pp_tidy <- pp |> 
  tidy(seed = 123) 

q = c(.025, .5, .975)
sim_auroc_pp_perf <- pp_tidy |> 
  group_by(model) |> 
  summarize(pp_median = quantile(posterior, probs = q[2]),
            pp_lower = quantile(posterior, probs = q[1]), 
            pp_upper = quantile(posterior, probs = q[3])) |> 
  mutate(model = factor(model, levels = c(0, 24, 72, 168, 336),
                        labels = c("0 lag", "24 lag", "72 lag", "168 lag", "336 lag"))) |> 
  arrange(model)

sim_auroc_pp_perf
```


The posterior probability distributions for auROC for each model are displayed below. The 95% credible intervals (CI) are depicted with a horizontal line. The vertical line represents the median posterior probability for auROC. The median auROCs from these posterior distributions were `r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "0 lag") |> pull(pp_median))` (baseline), `r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "24 lag") |> pull(pp_median))` (24 hour lag), `r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "72 lag") |> pull(pp_median))` (72 hour lag), `r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "168 lag") |> pull(pp_median))` (168 hour lag), and `r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "336 lag") |> pull(pp_median))` (336 hour lag). These values represent our best estimates for the magnitude of the auROC parameter for each model. The 95% Bayesian CI for the auROCs for these models were relatively narrow and did not contain 0.5: baseline[`r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "0 lag") |> pull(pp_lower))`-`r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "0 lag") |> pull(pp_upper))`], 24 hour lag[`r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "24 lag") |> pull(pp_lower))`-`r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "24 lag") |> pull(pp_upper))`], 72 hour lag[`r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "72 lag") |> pull(pp_lower))`-`r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "72 lag") |> pull(pp_upper))`], 168 hour lag[`r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "168 lag") |> pull(pp_lower))`-`r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "168 lag") |> pull(pp_upper))`], 336 hour lag[`r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "336 lag") |> pull(pp_lower))`-`r sprintf("%1.3f", sim_auroc_pp_perf |> filter(model == "336 lag") |> pull(pp_upper))`].

```{r}
#| echo: false

ci <- pp_tidy |> 
  summary() |> 
  mutate(model = factor(model, levels = c(0, 24, 72, 168, 336),
                        labels = c("0 lag", "24 lag", "72 lag", "168 lag", "336 lag")),
         y = 1400)

pp_tidy |> 
  mutate(model = factor(model, levels = c(0, 24, 72, 168, 336),
                        labels = c("0 lag", "24 lag", "72 lag", "168 lag", "336 lag"))) |>
  ggplot() + 
  geom_histogram(aes(x = posterior, fill = model), color = "black", alpha = .4, 
                 bins = 30) +
  geom_segment(mapping = aes(y = y+200, yend = y-200, x = mean, xend = mean),
               data = ci) +
  geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper),
                data = ci) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous("Posterior Probability", breaks = c(0, 500, 1000)) +
  xlab("Area Under ROC Curve") +
  theme_classic() +
  theme(legend.position = "none")
```



### Model Comparisons

We will perform two sets of Bayesian model comparisons. Our first set will compare each lagged model to our baseline model (no lag). We have demonstrated previously that this model performs well. Our contrasts will show whether we can predict lapses with similar performance with varying lags (predictions shifted into the future). Our four baseline model comparisons will consist of the following contrasts:    

Baseline Contrasts:     

1. No lag vs. 24 hour lag.    
2. No lag vs. 72 hour lag.    
3. No lag vs. 168 hour lag.    
4. No lag vs. 336 hour lag.    

Our second set of model comparisons will assess for performance differences between adjacent lags. This will allow us to determine the probability that the lagged models systematically degrade as predictions are made further into the future (i.e., longer lag time).    

Adjacent Lag Contrasts:    

1. 24 hour lag vs. 72 hour lag.    
2. 72 hour lag vs. 168 hour lag.    
3. 168 hour lag vs. 336 hour lag.  

For both sets of model comparisons we will determine the probability that the models' performances differed systematically from each other by regressing the auROCs (logit transformed) from the 30 test sets as a function of the contrast. We will set two random intercepts: one for the repeat, and another for the fold within repeat. We will also report 95% (equal-tailed) Bayesian CIs for the differences in performance associated with the Bayesian comparisons. We will plot the difference in the posterior probability distributions for each model comparison. We will also report the precise posterior probability for the difference in auROCs and the 95% Bayesian CIs. If there is a probability > 0.95 that the more lagged model's performance is worse, we will label the model contrast as significant.


#### Sample code with simulated data

Below we demonstrate our planned model comparisons with simulated data. We also show how we will write up our results using the simulated data. 


```{r}
ci_baseline <- pp |>
  contrast_models(list("0", "0", "0", "0"), 
                  list("24", "72", "168", "336")) |> 
  summary(size = 0) |> 
  mutate(contrast = factor(contrast, 
                           levels = c("0 vs 24", "0 vs 72", "0 vs 168", "0 vs 336"),
                           labels = c("0 vs. 24", "0 vs. 72", "0 vs. 168", "0 vs. 336")))

ci_median_baseline <- pp |> 
  contrast_models(list("0", "0", "0", "0"), 
                  list("24", "72", "168", "336")) |>  
  group_by(contrast) |> 
  summarize(median = quantile(difference, .5)) |> 
  mutate(contrast = factor(contrast, 
                           levels = c("0 vs. 24", "0 vs. 72", "0 vs. 168", "0 vs. 336")))


ci_baseline <- ci_baseline |> 
  left_join(ci_median_baseline, by = c("contrast")) 

ci_baseline |> 
  select(contrast, probability, median, lower, upper) |> 
  arrange(contrast)
```

The median decrease in auROC for the baseline vs. 24 hour lag model was `r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 24") |> pull(median))` (95% CI=[`r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 24") |> pull(lower))`-`r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 24") |> pull(upper))`]), yielding a significant probability of `r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 24") |> pull(probability))` that the lagged model had worse performance. The median decrease in auROC for the baseline vs. 72 hour model was `r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 72") |> pull(median))` (95% CI=[`r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 72") |> pull(lower))`-`r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 72") |> pull(upper))`]), yielding a significant probability of `r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 72") |> pull(probability))` that the lagged model had worse performance. The median increase in auROC for the baseline vs. 168 hour lag model was `r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 168") |> pull(median))` (95% CI=[`r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 168") |> pull(lower))`-`r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 168") |> pull(upper))`]), yielding a significant probability of`r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 168") |> pull(probability))` that the lagged model had worse performance. The median increase in auROC for the baseline vs. 336 hour lag model was `r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 336") |> pull(median))` (95% CI=[`r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 336") |> pull(lower))`-`r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 336") |> pull(upper))`]), yielding a significant probability of `r sprintf("%1.3f", ci_baseline |> filter(contrast == "0 vs. 336") |> pull(probability))` that the lagged model had worse performance. The plot below presents histograms of the posterior probability distributions for these model contrasts on auROC.

```{r}
#| echo: false

ci_baseline <- ci_baseline |> 
  mutate(y = 1200) 


pp |> 
  tidy(seed = 123) |>   
  group_by(model) |> 
  mutate(sample = row_number()) |> 
  ungroup() |> 
  pivot_wider(names_from = model, values_from = posterior) |> 
  mutate(`0 vs. 24` = `0` - `24`,
         `0 vs. 72` = `0` - `72`,
         `0 vs. 168` = `0` - `168`,
         `0 vs. 336` = `0` - `336`) |> 
  pivot_longer(cols = `0 vs. 24`:`0 vs. 336`,
               names_to = "contrast",
               values_to = "posterior") |>
  mutate(contrast = factor(contrast, 
                           levels = c("0 vs. 24", 
                                      "0 vs. 72", 
                                      "0 vs. 168",
                                      "0 vs. 336"))) |>
  ggplot() +
  geom_histogram(aes(x = posterior), 
                 color = "black", fill = "light grey", bins = 30) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_segment(mapping = aes(y = y+200, yend = y-200, x = median, xend = median), data = ci_baseline) +
  geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper), data = ci_baseline) +
  facet_wrap(~contrast, ncol = 1) +
  ylab("Posterior Probability") +
  xlab("Model Contrast for AUC") +
  theme_classic() +
  theme(legend.position = "none")
```

```{r}
ci_lag <- pp |>
  contrast_models(list("24", "72", "168"), 
                  list("72", "168", "336")) |> 
  summary(size = 0) |> 
  mutate(contrast = factor(contrast, 
                           levels = c("24 vs 72", "72 vs 168", "168 vs 336"),
                           labels = c("24 vs. 72", "72 vs. 168", "168 vs. 336")),
         y = 1200)

ci_median_lag <- pp |> 
  contrast_models(list("24", "72", "168"), 
                  list("72", "168", "336")) |>  
  group_by(contrast) |> 
  summarize(median = quantile(difference, .5)) |> 
  mutate(contrast = factor(contrast, 
                           levels = c("24 vs. 72", "72 vs. 168", "168 vs. 336")))

ci_lag <- ci_lag |> 
  left_join(ci_median_lag, by = c("contrast")) 

ci_lag |> 
  select(contrast, probability, median, lower, upper) |> 
  arrange(contrast)
```


The median decrease in auROC for the 24 hour vs. 72 hour lag model was `r sprintf("%1.3f", ci_lag |> filter(contrast == "24 vs. 72") |> pull(median))` (95% CI=[`r sprintf("%1.3f", ci_lag |> filter(contrast == "24 vs. 72") |> pull(lower))`-`r sprintf("%1.3f", ci_lag |> filter(contrast == "24 vs. 72") |> pull(upper))`]), yielding a non-significant probability of `r sprintf("%1.3f", ci_lag |> filter(contrast == "24 vs. 72") |> pull(probability))` that the 72 hour lag model had worse performance than the 24 hour lag model. The median decrease in auROC for the 72 hour vs. 168 hour lag model was `r sprintf("%1.3f", ci_lag |> filter(contrast == "72 vs. 168") |> pull(median))` (95% CI=[`r sprintf("%1.3f", ci_lag |> filter(contrast == "72 vs. 168") |> pull(lower))`-`r sprintf("%1.3f", ci_lag |> filter(contrast == "72 vs. 168") |> pull(upper))`]), yielding a significant probability of `r sprintf("%1.3f", ci_lag |> filter(contrast == "72 vs. 168") |> pull(probability))` that the 168 hour lag model had worse performance than the 72 hour lag model. The median decrease in auROC for the 168 hour vs. 336 hour lag model was `r sprintf("%1.3f", ci_lag |> filter(contrast == "168 vs. 336") |> pull(median))` (95% CI=[`r sprintf("%1.3f", ci_lag |> filter(contrast == "168 vs. 336") |> pull(lower))`-`r sprintf("%1.3f", ci_lag |> filter(contrast == "168 vs. 336") |> pull(upper))`]), yielding a significant probability of `r sprintf("%1.3f", ci_lag |> filter(contrast == "168 vs. 336") |> pull(probability))` that the 336 hour lag model had worse performance than the 168 hour lag model. The plot below presents histograms of the posterior probability distributions for these model contrasts on auROC.

```{r}
#| echo: false

ci_lag <- ci_lag |> 
  mutate(y = 800) 


pp |> 
  tidy(seed = 123) |>   
  group_by(model) |> 
  mutate(sample = row_number()) |> 
  ungroup() |> 
  pivot_wider(names_from = model, values_from = posterior) |> 
  mutate(`24 vs. 72` = `24` - `72`,
         `72 vs. 168` = `72` - `168`,
         `168 vs. 336` = `168` - `336`) |> 
  pivot_longer(cols = `24 vs. 72`:`168 vs. 336`,
               names_to = "contrast",
               values_to = "posterior") |>
  mutate(contrast = factor(contrast, 
                           levels = c("24 vs. 72", 
                                      "72 vs. 168",
                                      "168 vs. 336"))) |>
  ggplot() +
  geom_histogram(aes(x = posterior), 
                 color = "black", fill = "light grey", bins = 30) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_segment(mapping = aes(y = y+200, yend = y-200, x = median, xend = median), data = ci_lag) +
  geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper), data = ci_lag) +
  facet_wrap(~contrast, ncol = 1) +
  ylab("Posterior Probability") +
  xlab("Model Contrast for AUC") +
  theme_classic() +
  theme(legend.position = "none")
```



### Feature Importance Indices

We will calculate Shapley values from the 30 test sets to provide a description of the importance of categories of features across our five models in one of two ways.

1. If our best model is an XGBoost model, we will calculate Shapley values as we did in our previous study. We will use the `SHAPforxgboost` package designed specifically for XGBoost models. This method is preferred in that has low computational cost.   

2. However, if our best model configuration uses an algorithm other than XGBoost we will use a model-agnostic, more computationally expensive, method for calculating Shapley values. Specifically, we will calculate Shapley values using the `DALEX` package. This package calculates Shapley values by generating permutations of feature values, computing the model's predictions for each permutation, and then aggregating the differences between these predictions and the original prediction. This process, however, is time-intensive and computationally expensive.

Once Shapley values are calculated, we will average the three Shapley values for each observation for each feature (i.e., across the three repeats) to increase their stability. An inherent property of Shapley values is their additivity, allowing us to combine features into feature categories. We will create separate feature categories for each of the nine EMA questions, the rates of past alcohol use and missing surveys, the time of day and day of the week of the start of the prediction window, and the seven demographic variables included in the models.

To calculate the local (i.e., for each observation) importance for each category of features, we will add Shapley values across all features in a category, separately for each observation.  To calculate global importance for each feature category, we will average the absolute value of the Shapley values of all features in the category across all observations. These local and global importance scores based on Shapley values allow us to contextualize relative feature importance.  

We will plot the relative ordering of global feature importance using a bar plot. We will also use a sina plot to show the distribution of local feature importance for each observation. 











